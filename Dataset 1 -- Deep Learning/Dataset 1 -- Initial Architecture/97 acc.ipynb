{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf8ee932",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f663fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511af272",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04fd19b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emg1</th>\n",
       "      <th>Emg2</th>\n",
       "      <th>Emg3</th>\n",
       "      <th>Emg4</th>\n",
       "      <th>Emg5</th>\n",
       "      <th>Emg6</th>\n",
       "      <th>Emg7</th>\n",
       "      <th>Emg8</th>\n",
       "      <th>Emg9</th>\n",
       "      <th>Emg10</th>\n",
       "      <th>repetition</th>\n",
       "      <th>rerepetition</th>\n",
       "      <th>stimulus</th>\n",
       "      <th>restimulus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>108084</th>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0269</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80041</th>\n",
       "      <td>0.0708</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0513</td>\n",
       "      <td>0.1025</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0122</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25284</th>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0317</td>\n",
       "      <td>0.0757</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0098</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468278</th>\n",
       "      <td>0.4956</td>\n",
       "      <td>0.3125</td>\n",
       "      <td>0.0806</td>\n",
       "      <td>0.0586</td>\n",
       "      <td>0.0757</td>\n",
       "      <td>0.0269</td>\n",
       "      <td>0.9839</td>\n",
       "      <td>1.3818</td>\n",
       "      <td>0.1514</td>\n",
       "      <td>0.0342</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369326</th>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0146</td>\n",
       "      <td>0.0684</td>\n",
       "      <td>0.1367</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Emg1    Emg2    Emg3    Emg4    Emg5    Emg6    Emg7    Emg8  \\\n",
       "108084  0.0024  0.0024  0.0024  0.0024  0.0024  0.0024  0.0024  0.0269   \n",
       "80041   0.0708  0.0024  0.0024  0.0024  0.0024  0.0024  0.0513  0.1025   \n",
       "25284   0.0024  0.0024  0.0024  0.0024  0.0024  0.0024  0.0317  0.0757   \n",
       "468278  0.4956  0.3125  0.0806  0.0586  0.0757  0.0269  0.9839  1.3818   \n",
       "369326  0.0024  0.0024  0.0024  0.0024  0.0024  0.0024  0.0146  0.0684   \n",
       "\n",
       "          Emg9   Emg10  repetition  rerepetition  stimulus  restimulus  \n",
       "108084  0.0024  0.0024           9             0         1           1  \n",
       "80041   0.0024  0.0122           6             0        10           0  \n",
       "25284   0.0024  0.0098           0             0         0           0  \n",
       "468278  0.1514  0.0342           7             7        23          23  \n",
       "369326  0.1367  0.0073           0             0         0           0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = pd.read_excel('Dataset 1 Patient 1.xlsx')\n",
    "raw_data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80369ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 471483 entries, 0 to 471482\n",
      "Data columns (total 14 columns):\n",
      " #   Column        Non-Null Count   Dtype  \n",
      "---  ------        --------------   -----  \n",
      " 0   Emg1          471483 non-null  float64\n",
      " 1   Emg2          471483 non-null  float64\n",
      " 2   Emg3          471483 non-null  float64\n",
      " 3   Emg4          471483 non-null  float64\n",
      " 4   Emg5          471483 non-null  float64\n",
      " 5   Emg6          471483 non-null  float64\n",
      " 6   Emg7          471483 non-null  float64\n",
      " 7   Emg8          471483 non-null  float64\n",
      " 8   Emg9          471483 non-null  float64\n",
      " 9   Emg10         471483 non-null  float64\n",
      " 10  repetition    471483 non-null  int64  \n",
      " 11  rerepetition  471483 non-null  int64  \n",
      " 12  stimulus      471483 non-null  int64  \n",
      " 13  restimulus    471483 non-null  int64  \n",
      "dtypes: float64(10), int64(4)\n",
      "memory usage: 50.4 MB\n"
     ]
    }
   ],
   "source": [
    "raw_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "109445f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emg1</th>\n",
       "      <th>Emg2</th>\n",
       "      <th>Emg3</th>\n",
       "      <th>Emg4</th>\n",
       "      <th>Emg5</th>\n",
       "      <th>Emg6</th>\n",
       "      <th>Emg7</th>\n",
       "      <th>Emg8</th>\n",
       "      <th>Emg9</th>\n",
       "      <th>Emg10</th>\n",
       "      <th>repetition</th>\n",
       "      <th>rerepetition</th>\n",
       "      <th>stimulus</th>\n",
       "      <th>restimulus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>471483.000000</td>\n",
       "      <td>471483.000000</td>\n",
       "      <td>471483.000000</td>\n",
       "      <td>471483.000000</td>\n",
       "      <td>471483.000000</td>\n",
       "      <td>471483.000000</td>\n",
       "      <td>471483.000000</td>\n",
       "      <td>471483.000000</td>\n",
       "      <td>471483.000000</td>\n",
       "      <td>471483.000000</td>\n",
       "      <td>471483.000000</td>\n",
       "      <td>471483.000000</td>\n",
       "      <td>471483.000000</td>\n",
       "      <td>471483.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.129657</td>\n",
       "      <td>0.122672</td>\n",
       "      <td>0.123409</td>\n",
       "      <td>0.044321</td>\n",
       "      <td>0.012722</td>\n",
       "      <td>0.014612</td>\n",
       "      <td>0.221796</td>\n",
       "      <td>0.233414</td>\n",
       "      <td>0.107259</td>\n",
       "      <td>0.072334</td>\n",
       "      <td>3.136047</td>\n",
       "      <td>2.113255</td>\n",
       "      <td>5.562892</td>\n",
       "      <td>4.570513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.286859</td>\n",
       "      <td>0.322911</td>\n",
       "      <td>0.337717</td>\n",
       "      <td>0.167680</td>\n",
       "      <td>0.032359</td>\n",
       "      <td>0.042109</td>\n",
       "      <td>0.476014</td>\n",
       "      <td>0.353467</td>\n",
       "      <td>0.233386</td>\n",
       "      <td>0.156993</td>\n",
       "      <td>3.480664</td>\n",
       "      <td>3.212682</td>\n",
       "      <td>6.575838</td>\n",
       "      <td>6.427040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.012200</td>\n",
       "      <td>0.063500</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.009800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.017100</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.051300</td>\n",
       "      <td>0.112300</td>\n",
       "      <td>0.007300</td>\n",
       "      <td>0.039100</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.114700</td>\n",
       "      <td>0.046400</td>\n",
       "      <td>0.058600</td>\n",
       "      <td>0.007300</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.190400</td>\n",
       "      <td>0.244100</td>\n",
       "      <td>0.136700</td>\n",
       "      <td>0.065900</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.665500</td>\n",
       "      <td>4.663100</td>\n",
       "      <td>4.658200</td>\n",
       "      <td>4.663100</td>\n",
       "      <td>0.876500</td>\n",
       "      <td>1.484400</td>\n",
       "      <td>4.663100</td>\n",
       "      <td>4.665500</td>\n",
       "      <td>4.660600</td>\n",
       "      <td>4.628900</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>23.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Emg1           Emg2           Emg3           Emg4  \\\n",
       "count  471483.000000  471483.000000  471483.000000  471483.000000   \n",
       "mean        0.129657       0.122672       0.123409       0.044321   \n",
       "std         0.286859       0.322911       0.337717       0.167680   \n",
       "min         0.002400       0.000000       0.002400       0.000000   \n",
       "25%         0.002400       0.002400       0.002400       0.002400   \n",
       "50%         0.017100       0.002400       0.002400       0.002400   \n",
       "75%         0.114700       0.046400       0.058600       0.007300   \n",
       "max         4.665500       4.663100       4.658200       4.663100   \n",
       "\n",
       "                Emg5           Emg6           Emg7           Emg8  \\\n",
       "count  471483.000000  471483.000000  471483.000000  471483.000000   \n",
       "mean        0.012722       0.014612       0.221796       0.233414   \n",
       "std         0.032359       0.042109       0.476014       0.353467   \n",
       "min         0.002400       0.000000       0.002400       0.002400   \n",
       "25%         0.002400       0.002400       0.012200       0.063500   \n",
       "50%         0.002400       0.002400       0.051300       0.112300   \n",
       "75%         0.002400       0.002400       0.190400       0.244100   \n",
       "max         0.876500       1.484400       4.663100       4.665500   \n",
       "\n",
       "                Emg9          Emg10     repetition   rerepetition  \\\n",
       "count  471483.000000  471483.000000  471483.000000  471483.000000   \n",
       "mean        0.107259       0.072334       3.136047       2.113255   \n",
       "std         0.233386       0.156993       3.480664       3.212682   \n",
       "min         0.000000       0.002400       0.000000       0.000000   \n",
       "25%         0.002400       0.009800       0.000000       0.000000   \n",
       "50%         0.007300       0.039100       2.000000       0.000000   \n",
       "75%         0.136700       0.065900       6.000000       4.000000   \n",
       "max         4.660600       4.628900      10.000000      10.000000   \n",
       "\n",
       "            stimulus     restimulus  \n",
       "count  471483.000000  471483.000000  \n",
       "mean        5.562892       4.570513  \n",
       "std         6.575838       6.427040  \n",
       "min         0.000000       0.000000  \n",
       "25%         0.000000       0.000000  \n",
       "50%         3.000000       0.000000  \n",
       "75%        10.000000       9.000000  \n",
       "max        23.000000      23.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0b9a2e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Dependent values and their counts :\n",
      "0     202625\n",
      "2      15538\n",
      "12     15532\n",
      "8      15531\n",
      "7      15518\n",
      "4      15516\n",
      "11     15514\n",
      "5      15492\n",
      "9      15492\n",
      "10     15477\n",
      "1      15476\n",
      "3      15469\n",
      "6      15469\n",
      "14     10361\n",
      "13     10360\n",
      "17     10346\n",
      "15     10334\n",
      "16     10320\n",
      "18      5210\n",
      "20      5202\n",
      "19      5189\n",
      "21      5185\n",
      "23      5166\n",
      "22      5161\n",
      "Name: stimulus, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique Dependent values and their counts :\")\n",
    "print(raw_data[\"stimulus\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54420ec4",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a811198b",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_names = raw_data[ (raw_data['stimulus'] != raw_data['restimulus'])].index\n",
    "raw_data.drop(index_names, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "556cd92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_names = raw_data[ (raw_data['repetition'] != raw_data['rerepetition'])].index\n",
    "raw_data.drop(index_names, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c91744f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = raw_data.iloc[:,0:10]\n",
    "y = raw_data.stimulus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152f7160",
   "metadata": {},
   "source": [
    "# One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfb77d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be64bf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding for categorical labels\n",
    "import keras\n",
    "from keras import utils as np_utils\n",
    "y = keras.utils.np_utils.to_categorical(y, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7358f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3088a1",
   "metadata": {},
   "source": [
    "# Standard Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5bd820f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "standardscaler=StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a118694",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pd.DataFrame(standardscaler.fit_transform(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6e2f38b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.273175</td>\n",
       "      <td>-0.420358</td>\n",
       "      <td>-0.402043</td>\n",
       "      <td>-0.277718</td>\n",
       "      <td>-0.355235</td>\n",
       "      <td>-0.163107</td>\n",
       "      <td>-0.495774</td>\n",
       "      <td>-0.564693</td>\n",
       "      <td>-0.498765</td>\n",
       "      <td>-0.275575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.304453</td>\n",
       "      <td>-0.420358</td>\n",
       "      <td>-0.402043</td>\n",
       "      <td>-0.277718</td>\n",
       "      <td>-0.355235</td>\n",
       "      <td>-0.268859</td>\n",
       "      <td>-0.495774</td>\n",
       "      <td>-0.583680</td>\n",
       "      <td>-0.498765</td>\n",
       "      <td>-0.305083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.312113</td>\n",
       "      <td>-0.420358</td>\n",
       "      <td>-0.402043</td>\n",
       "      <td>-0.277718</td>\n",
       "      <td>-0.355235</td>\n",
       "      <td>-0.268859</td>\n",
       "      <td>-0.495774</td>\n",
       "      <td>-0.589922</td>\n",
       "      <td>-0.498765</td>\n",
       "      <td>-0.334591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.312113</td>\n",
       "      <td>-0.420358</td>\n",
       "      <td>-0.402043</td>\n",
       "      <td>-0.277718</td>\n",
       "      <td>-0.355235</td>\n",
       "      <td>-0.268859</td>\n",
       "      <td>-0.495774</td>\n",
       "      <td>-0.602667</td>\n",
       "      <td>-0.498765</td>\n",
       "      <td>-0.378552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.335731</td>\n",
       "      <td>-0.420358</td>\n",
       "      <td>-0.402043</td>\n",
       "      <td>-0.277718</td>\n",
       "      <td>-0.355235</td>\n",
       "      <td>-0.322813</td>\n",
       "      <td>-0.495774</td>\n",
       "      <td>-0.596424</td>\n",
       "      <td>-0.498765</td>\n",
       "      <td>-0.393607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378530</th>\n",
       "      <td>-0.483823</td>\n",
       "      <td>-0.420358</td>\n",
       "      <td>-0.402043</td>\n",
       "      <td>-0.277718</td>\n",
       "      <td>-0.355235</td>\n",
       "      <td>-0.322813</td>\n",
       "      <td>-0.495774</td>\n",
       "      <td>-0.545445</td>\n",
       "      <td>-0.007433</td>\n",
       "      <td>-0.467075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378531</th>\n",
       "      <td>-0.483823</td>\n",
       "      <td>-0.420358</td>\n",
       "      <td>-0.402043</td>\n",
       "      <td>-0.277718</td>\n",
       "      <td>-0.355235</td>\n",
       "      <td>-0.268859</td>\n",
       "      <td>-0.495774</td>\n",
       "      <td>-0.558190</td>\n",
       "      <td>0.012680</td>\n",
       "      <td>-0.467075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378532</th>\n",
       "      <td>-0.483823</td>\n",
       "      <td>-0.420358</td>\n",
       "      <td>-0.402043</td>\n",
       "      <td>-0.277718</td>\n",
       "      <td>-0.355235</td>\n",
       "      <td>-0.322813</td>\n",
       "      <td>-0.495774</td>\n",
       "      <td>-0.558190</td>\n",
       "      <td>0.012680</td>\n",
       "      <td>-0.467075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378533</th>\n",
       "      <td>-0.483823</td>\n",
       "      <td>-0.420358</td>\n",
       "      <td>-0.402043</td>\n",
       "      <td>-0.277718</td>\n",
       "      <td>-0.355235</td>\n",
       "      <td>-0.322813</td>\n",
       "      <td>-0.495774</td>\n",
       "      <td>-0.564693</td>\n",
       "      <td>0.022532</td>\n",
       "      <td>-0.467075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378534</th>\n",
       "      <td>-0.483823</td>\n",
       "      <td>-0.420358</td>\n",
       "      <td>-0.402043</td>\n",
       "      <td>-0.277718</td>\n",
       "      <td>-0.355235</td>\n",
       "      <td>-0.322813</td>\n",
       "      <td>-0.495774</td>\n",
       "      <td>-0.577437</td>\n",
       "      <td>0.022532</td>\n",
       "      <td>-0.467075</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>378535 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0         1         2         3         4         5         6  \\\n",
       "0      -0.273175 -0.420358 -0.402043 -0.277718 -0.355235 -0.163107 -0.495774   \n",
       "1      -0.304453 -0.420358 -0.402043 -0.277718 -0.355235 -0.268859 -0.495774   \n",
       "2      -0.312113 -0.420358 -0.402043 -0.277718 -0.355235 -0.268859 -0.495774   \n",
       "3      -0.312113 -0.420358 -0.402043 -0.277718 -0.355235 -0.268859 -0.495774   \n",
       "4      -0.335731 -0.420358 -0.402043 -0.277718 -0.355235 -0.322813 -0.495774   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "378530 -0.483823 -0.420358 -0.402043 -0.277718 -0.355235 -0.322813 -0.495774   \n",
       "378531 -0.483823 -0.420358 -0.402043 -0.277718 -0.355235 -0.268859 -0.495774   \n",
       "378532 -0.483823 -0.420358 -0.402043 -0.277718 -0.355235 -0.322813 -0.495774   \n",
       "378533 -0.483823 -0.420358 -0.402043 -0.277718 -0.355235 -0.322813 -0.495774   \n",
       "378534 -0.483823 -0.420358 -0.402043 -0.277718 -0.355235 -0.322813 -0.495774   \n",
       "\n",
       "               7         8         9  \n",
       "0      -0.564693 -0.498765 -0.275575  \n",
       "1      -0.583680 -0.498765 -0.305083  \n",
       "2      -0.589922 -0.498765 -0.334591  \n",
       "3      -0.602667 -0.498765 -0.378552  \n",
       "4      -0.596424 -0.498765 -0.393607  \n",
       "...          ...       ...       ...  \n",
       "378530 -0.545445 -0.007433 -0.467075  \n",
       "378531 -0.558190  0.012680 -0.467075  \n",
       "378532 -0.558190  0.012680 -0.467075  \n",
       "378533 -0.564693  0.022532 -0.467075  \n",
       "378534 -0.577437  0.022532 -0.467075  \n",
       "\n",
       "[378535 rows x 10 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65cfb75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data for training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(sc, y, test_size = 0.2, random_state = 39)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba47781",
   "metadata": {},
   "source": [
    "# Deep Learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d426e2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adadelta, Adagrad, Adamax, Nadam, Ftrl\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd0ef208",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LeakyReLU\n",
    "LeakyReLU = LeakyReLU(alpha=0.1)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Convolution1D, Dropout\n",
    "from keras.initializers import random_uniform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97d61bc",
   "metadata": {},
   "source": [
    "# 1. Define Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd77bf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 24\n",
    "input_dim = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fbdce465",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential    \n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "model.add(Dense(48, input_dim=input_dim, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(96, activation='softmax'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "\n",
    "model.add(Dense(192, activation='softmax'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(384, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(768, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(24, activation='softmax'))\n",
    "\n",
    "model.add(Dense(768, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(384, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(192, activation='softmax'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(96, activation='softmax'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "\n",
    "model.add(Dense(48, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1dd33478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 48)                528       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 48)               192       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 96)                4704      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 96)               384       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 192)               18624     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 192)              768       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 384)               74112     \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 384)              1536      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 768)               295680    \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 768)              3072      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 24)                18456     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 768)               19200     \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 768)              3072      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 384)               295296    \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 384)              1536      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 192)               73920     \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 192)              768       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 96)                18528     \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 96)               384       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 48)                4656      \n",
      "                                                                 \n",
      " batch_normalization_9 (Batc  (None, 48)               192       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 24)                1176      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 836,784\n",
      "Trainable params: 830,832\n",
      "Non-trainable params: 5,952\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3196ac97",
   "metadata": {},
   "source": [
    "# 2. Compile Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7a2d558b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c1d2b36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "plot_model(model, 'EMG_ANN', show_shapes = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3358b592",
   "metadata": {},
   "source": [
    "# 3. Fit Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0ed4ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "75/75 [==============================] - 4s 20ms/step - loss: 1.6772 - accuracy: 0.5721 - val_loss: 2.9464 - val_accuracy: 0.5284\n",
      "Epoch 2/300\n",
      "75/75 [==============================] - 1s 17ms/step - loss: 1.1762 - accuracy: 0.6830 - val_loss: 2.7853 - val_accuracy: 0.5284\n",
      "Epoch 3/300\n",
      "75/75 [==============================] - 1s 17ms/step - loss: 0.8974 - accuracy: 0.7549 - val_loss: 2.6125 - val_accuracy: 0.5284\n",
      "Epoch 4/300\n",
      "75/75 [==============================] - 1s 17ms/step - loss: 0.7326 - accuracy: 0.7977 - val_loss: 2.4661 - val_accuracy: 0.5284\n",
      "Epoch 5/300\n",
      "75/75 [==============================] - 1s 17ms/step - loss: 0.6414 - accuracy: 0.8205 - val_loss: 2.3730 - val_accuracy: 0.5284\n",
      "Epoch 6/300\n",
      "75/75 [==============================] - 1s 17ms/step - loss: 0.5838 - accuracy: 0.8342 - val_loss: 2.3073 - val_accuracy: 0.5284\n",
      "Epoch 7/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.5445 - accuracy: 0.8443 - val_loss: 2.2349 - val_accuracy: 0.5284\n",
      "Epoch 8/300\n",
      "75/75 [==============================] - 1s 17ms/step - loss: 0.5087 - accuracy: 0.8537 - val_loss: 2.1695 - val_accuracy: 0.5335\n",
      "Epoch 9/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.4790 - accuracy: 0.8619 - val_loss: 1.9862 - val_accuracy: 0.5580\n",
      "Epoch 10/300\n",
      "75/75 [==============================] - 1s 17ms/step - loss: 0.4552 - accuracy: 0.8685 - val_loss: 1.5503 - val_accuracy: 0.6526\n",
      "Epoch 11/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.4394 - accuracy: 0.8726 - val_loss: 1.0532 - val_accuracy: 0.6954\n",
      "Epoch 12/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.4165 - accuracy: 0.8792 - val_loss: 0.5335 - val_accuracy: 0.8527\n",
      "Epoch 13/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.4010 - accuracy: 0.8839 - val_loss: 0.4594 - val_accuracy: 0.8699\n",
      "Epoch 14/300\n",
      "75/75 [==============================] - 1s 17ms/step - loss: 0.3960 - accuracy: 0.8848 - val_loss: 0.4313 - val_accuracy: 0.8762\n",
      "Epoch 15/300\n",
      "75/75 [==============================] - 1s 17ms/step - loss: 0.3846 - accuracy: 0.8882 - val_loss: 0.4450 - val_accuracy: 0.8740\n",
      "Epoch 16/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.3652 - accuracy: 0.8942 - val_loss: 0.4275 - val_accuracy: 0.8800\n",
      "Epoch 17/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.3551 - accuracy: 0.8970 - val_loss: 0.4196 - val_accuracy: 0.8830\n",
      "Epoch 18/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.3404 - accuracy: 0.9009 - val_loss: 0.4035 - val_accuracy: 0.8886\n",
      "Epoch 19/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.3420 - accuracy: 0.9004 - val_loss: 0.4169 - val_accuracy: 0.8847\n",
      "Epoch 20/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.3306 - accuracy: 0.9034 - val_loss: 0.3988 - val_accuracy: 0.8862\n",
      "Epoch 21/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.3208 - accuracy: 0.9068 - val_loss: 0.3858 - val_accuracy: 0.8939\n",
      "Epoch 22/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.3112 - accuracy: 0.9093 - val_loss: 0.4018 - val_accuracy: 0.8868\n",
      "Epoch 23/300\n",
      "75/75 [==============================] - 1s 17ms/step - loss: 0.3090 - accuracy: 0.9102 - val_loss: 0.4003 - val_accuracy: 0.8922\n",
      "Epoch 24/300\n",
      "75/75 [==============================] - 1s 17ms/step - loss: 0.3037 - accuracy: 0.9119 - val_loss: 0.3629 - val_accuracy: 0.8983\n",
      "Epoch 25/300\n",
      "75/75 [==============================] - 1s 17ms/step - loss: 0.2920 - accuracy: 0.9153 - val_loss: 0.4092 - val_accuracy: 0.8866\n",
      "Epoch 26/300\n",
      "75/75 [==============================] - 1s 17ms/step - loss: 0.2874 - accuracy: 0.9165 - val_loss: 0.3694 - val_accuracy: 0.8980\n",
      "Epoch 27/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.2864 - accuracy: 0.9165 - val_loss: 0.3787 - val_accuracy: 0.8986\n",
      "Epoch 28/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.2813 - accuracy: 0.9182 - val_loss: 0.3603 - val_accuracy: 0.9027\n",
      "Epoch 29/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.2749 - accuracy: 0.9200 - val_loss: 0.3750 - val_accuracy: 0.8980\n",
      "Epoch 30/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.2724 - accuracy: 0.9206 - val_loss: 0.3554 - val_accuracy: 0.9018\n",
      "Epoch 31/300\n",
      "75/75 [==============================] - 1s 17ms/step - loss: 0.2665 - accuracy: 0.9226 - val_loss: 0.3416 - val_accuracy: 0.9064\n",
      "Epoch 32/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.2598 - accuracy: 0.9247 - val_loss: 0.3309 - val_accuracy: 0.9086\n",
      "Epoch 33/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.2612 - accuracy: 0.9243 - val_loss: 0.3596 - val_accuracy: 0.8995\n",
      "Epoch 34/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.2589 - accuracy: 0.9247 - val_loss: 0.3464 - val_accuracy: 0.9036\n",
      "Epoch 35/300\n",
      "75/75 [==============================] - 1s 17ms/step - loss: 0.2528 - accuracy: 0.9268 - val_loss: 0.3543 - val_accuracy: 0.9036\n",
      "Epoch 36/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.2484 - accuracy: 0.9278 - val_loss: 0.3412 - val_accuracy: 0.9074\n",
      "Epoch 37/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.2441 - accuracy: 0.9293 - val_loss: 0.3350 - val_accuracy: 0.9086\n",
      "Epoch 38/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.2434 - accuracy: 0.9298 - val_loss: 0.3326 - val_accuracy: 0.9110\n",
      "Epoch 39/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.2378 - accuracy: 0.9313 - val_loss: 0.3423 - val_accuracy: 0.9077\n",
      "Epoch 40/300\n",
      "75/75 [==============================] - 1s 17ms/step - loss: 0.2383 - accuracy: 0.9310 - val_loss: 0.3333 - val_accuracy: 0.9094\n",
      "Epoch 41/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.2407 - accuracy: 0.9305 - val_loss: 0.3524 - val_accuracy: 0.9039\n",
      "Epoch 42/300\n",
      "75/75 [==============================] - 1s 17ms/step - loss: 0.2379 - accuracy: 0.9312 - val_loss: 0.3807 - val_accuracy: 0.9032\n",
      "Epoch 43/300\n",
      "75/75 [==============================] - 1s 17ms/step - loss: 0.2347 - accuracy: 0.9319 - val_loss: 0.3167 - val_accuracy: 0.9130\n",
      "Epoch 44/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.2262 - accuracy: 0.9345 - val_loss: 0.3172 - val_accuracy: 0.9143\n",
      "Epoch 45/300\n",
      "75/75 [==============================] - 1s 17ms/step - loss: 0.2256 - accuracy: 0.9347 - val_loss: 0.3218 - val_accuracy: 0.9103\n",
      "Epoch 46/300\n",
      "75/75 [==============================] - 1s 17ms/step - loss: 0.2191 - accuracy: 0.9367 - val_loss: 0.3127 - val_accuracy: 0.9166\n",
      "Epoch 47/300\n",
      "75/75 [==============================] - 1s 17ms/step - loss: 0.2203 - accuracy: 0.9359 - val_loss: 0.3386 - val_accuracy: 0.9134\n",
      "Epoch 48/300\n",
      "75/75 [==============================] - 1s 17ms/step - loss: 0.2221 - accuracy: 0.9358 - val_loss: 0.3352 - val_accuracy: 0.9118\n",
      "Epoch 49/300\n",
      "75/75 [==============================] - 1s 17ms/step - loss: 0.2135 - accuracy: 0.9386 - val_loss: 0.3075 - val_accuracy: 0.9170\n",
      "Epoch 50/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.2108 - accuracy: 0.9398 - val_loss: 0.3091 - val_accuracy: 0.9158\n",
      "Epoch 51/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.2104 - accuracy: 0.9393 - val_loss: 0.3202 - val_accuracy: 0.9159\n",
      "Epoch 52/300\n",
      "75/75 [==============================] - 1s 17ms/step - loss: 0.2119 - accuracy: 0.9390 - val_loss: 0.3279 - val_accuracy: 0.9114\n",
      "Epoch 53/300\n",
      "75/75 [==============================] - 1s 17ms/step - loss: 0.2112 - accuracy: 0.9390 - val_loss: 0.3128 - val_accuracy: 0.9160\n",
      "Epoch 54/300\n",
      "75/75 [==============================] - 1s 17ms/step - loss: 0.2064 - accuracy: 0.9408 - val_loss: 0.3036 - val_accuracy: 0.9192\n",
      "Epoch 55/300\n",
      "75/75 [==============================] - 1s 17ms/step - loss: 0.2028 - accuracy: 0.9415 - val_loss: 0.3176 - val_accuracy: 0.9167\n",
      "Epoch 56/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.2058 - accuracy: 0.9407 - val_loss: 0.3193 - val_accuracy: 0.9190\n",
      "Epoch 57/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1997 - accuracy: 0.9423 - val_loss: 0.3015 - val_accuracy: 0.9190\n",
      "Epoch 58/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1977 - accuracy: 0.9433 - val_loss: 0.3110 - val_accuracy: 0.9174\n",
      "Epoch 59/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.2002 - accuracy: 0.9426 - val_loss: 0.3048 - val_accuracy: 0.9192\n",
      "Epoch 60/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1971 - accuracy: 0.9433 - val_loss: 0.3169 - val_accuracy: 0.9155\n",
      "Epoch 61/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1927 - accuracy: 0.9448 - val_loss: 0.3043 - val_accuracy: 0.9197\n",
      "Epoch 62/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1925 - accuracy: 0.9448 - val_loss: 0.3046 - val_accuracy: 0.9208\n",
      "Epoch 63/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1879 - accuracy: 0.9464 - val_loss: 0.3056 - val_accuracy: 0.9205\n",
      "Epoch 64/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1915 - accuracy: 0.9447 - val_loss: 0.3132 - val_accuracy: 0.9183\n",
      "Epoch 65/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1920 - accuracy: 0.9449 - val_loss: 0.3024 - val_accuracy: 0.9212\n",
      "Epoch 66/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1904 - accuracy: 0.9452 - val_loss: 0.3252 - val_accuracy: 0.9144\n",
      "Epoch 67/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1859 - accuracy: 0.9467 - val_loss: 0.3064 - val_accuracy: 0.9216\n",
      "Epoch 68/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1864 - accuracy: 0.9466 - val_loss: 0.2966 - val_accuracy: 0.9222\n",
      "Epoch 69/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1828 - accuracy: 0.9474 - val_loss: 0.3066 - val_accuracy: 0.9143\n",
      "Epoch 70/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1771 - accuracy: 0.9493 - val_loss: 0.2946 - val_accuracy: 0.9247\n",
      "Epoch 71/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1776 - accuracy: 0.9495 - val_loss: 0.2859 - val_accuracy: 0.9255\n",
      "Epoch 72/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1792 - accuracy: 0.9488 - val_loss: 0.2999 - val_accuracy: 0.9224\n",
      "Epoch 73/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1836 - accuracy: 0.9476 - val_loss: 0.3026 - val_accuracy: 0.9233\n",
      "Epoch 74/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1771 - accuracy: 0.9490 - val_loss: 0.2935 - val_accuracy: 0.9227\n",
      "Epoch 75/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1749 - accuracy: 0.9501 - val_loss: 0.2968 - val_accuracy: 0.9224\n",
      "Epoch 76/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1785 - accuracy: 0.9488 - val_loss: 0.2963 - val_accuracy: 0.9246\n",
      "Epoch 77/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1767 - accuracy: 0.9492 - val_loss: 0.2925 - val_accuracy: 0.9261\n",
      "Epoch 78/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1774 - accuracy: 0.9492 - val_loss: 0.3055 - val_accuracy: 0.9226\n",
      "Epoch 79/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1762 - accuracy: 0.9493 - val_loss: 0.2996 - val_accuracy: 0.9222\n",
      "Epoch 80/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1693 - accuracy: 0.9515 - val_loss: 0.2822 - val_accuracy: 0.9272\n",
      "Epoch 81/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1664 - accuracy: 0.9528 - val_loss: 0.2885 - val_accuracy: 0.9261\n",
      "Epoch 82/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1710 - accuracy: 0.9511 - val_loss: 0.3184 - val_accuracy: 0.9207\n",
      "Epoch 83/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1772 - accuracy: 0.9490 - val_loss: 0.3054 - val_accuracy: 0.9232\n",
      "Epoch 84/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1653 - accuracy: 0.9528 - val_loss: 0.2884 - val_accuracy: 0.9267\n",
      "Epoch 85/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1656 - accuracy: 0.9529 - val_loss: 0.2955 - val_accuracy: 0.9253\n",
      "Epoch 86/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1663 - accuracy: 0.9526 - val_loss: 0.2891 - val_accuracy: 0.9261\n",
      "Epoch 87/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1654 - accuracy: 0.9526 - val_loss: 0.2870 - val_accuracy: 0.9250\n",
      "Epoch 88/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1783 - accuracy: 0.9483 - val_loss: 0.3247 - val_accuracy: 0.9093\n",
      "Epoch 89/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1625 - accuracy: 0.9533 - val_loss: 0.2905 - val_accuracy: 0.9255\n",
      "Epoch 90/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1613 - accuracy: 0.9539 - val_loss: 0.2891 - val_accuracy: 0.9283\n",
      "Epoch 91/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1595 - accuracy: 0.9540 - val_loss: 0.2839 - val_accuracy: 0.9270\n",
      "Epoch 92/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1591 - accuracy: 0.9544 - val_loss: 0.2894 - val_accuracy: 0.9269\n",
      "Epoch 93/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1630 - accuracy: 0.9531 - val_loss: 0.3009 - val_accuracy: 0.9271\n",
      "Epoch 94/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1612 - accuracy: 0.9539 - val_loss: 0.3181 - val_accuracy: 0.9206\n",
      "Epoch 95/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1622 - accuracy: 0.9533 - val_loss: 0.3102 - val_accuracy: 0.9237\n",
      "Epoch 96/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1624 - accuracy: 0.9534 - val_loss: 0.3048 - val_accuracy: 0.9246\n",
      "Epoch 97/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1579 - accuracy: 0.9548 - val_loss: 0.3060 - val_accuracy: 0.9255\n",
      "Epoch 98/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1530 - accuracy: 0.9564 - val_loss: 0.2977 - val_accuracy: 0.9239\n",
      "Epoch 99/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1542 - accuracy: 0.9560 - val_loss: 0.2938 - val_accuracy: 0.9248\n",
      "Epoch 100/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1545 - accuracy: 0.9559 - val_loss: 0.3185 - val_accuracy: 0.9203\n",
      "Epoch 101/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1509 - accuracy: 0.9569 - val_loss: 0.2932 - val_accuracy: 0.9275\n",
      "Epoch 102/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1490 - accuracy: 0.9574 - val_loss: 0.2899 - val_accuracy: 0.9294\n",
      "Epoch 103/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1538 - accuracy: 0.9559 - val_loss: 0.2944 - val_accuracy: 0.9266\n",
      "Epoch 104/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1518 - accuracy: 0.9568 - val_loss: 0.2901 - val_accuracy: 0.9300\n",
      "Epoch 105/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1498 - accuracy: 0.9571 - val_loss: 0.2839 - val_accuracy: 0.9296\n",
      "Epoch 106/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1546 - accuracy: 0.9556 - val_loss: 0.2927 - val_accuracy: 0.9282\n",
      "Epoch 107/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1495 - accuracy: 0.9573 - val_loss: 0.3109 - val_accuracy: 0.9212\n",
      "Epoch 108/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1467 - accuracy: 0.9580 - val_loss: 0.2895 - val_accuracy: 0.9297\n",
      "Epoch 109/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1480 - accuracy: 0.9573 - val_loss: 0.3133 - val_accuracy: 0.9242\n",
      "Epoch 110/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1520 - accuracy: 0.9564 - val_loss: 0.2870 - val_accuracy: 0.9292\n",
      "Epoch 111/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1468 - accuracy: 0.9580 - val_loss: 0.2880 - val_accuracy: 0.9287\n",
      "Epoch 112/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1465 - accuracy: 0.9581 - val_loss: 0.3120 - val_accuracy: 0.9239\n",
      "Epoch 113/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1471 - accuracy: 0.9579 - val_loss: 0.3203 - val_accuracy: 0.9194\n",
      "Epoch 114/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1473 - accuracy: 0.9580 - val_loss: 0.2836 - val_accuracy: 0.9302\n",
      "Epoch 115/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1449 - accuracy: 0.9586 - val_loss: 0.2855 - val_accuracy: 0.9305\n",
      "Epoch 116/300\n",
      "75/75 [==============================] - 1s 17ms/step - loss: 0.1434 - accuracy: 0.9593 - val_loss: 0.2858 - val_accuracy: 0.9307\n",
      "Epoch 117/300\n",
      "75/75 [==============================] - 1s 17ms/step - loss: 0.1404 - accuracy: 0.9600 - val_loss: 0.2870 - val_accuracy: 0.9297\n",
      "Epoch 118/300\n",
      "75/75 [==============================] - 1s 17ms/step - loss: 0.1410 - accuracy: 0.9598 - val_loss: 0.2827 - val_accuracy: 0.9315\n",
      "Epoch 119/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1395 - accuracy: 0.9604 - val_loss: 0.2792 - val_accuracy: 0.9330\n",
      "Epoch 120/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1423 - accuracy: 0.9597 - val_loss: 0.2941 - val_accuracy: 0.9296\n",
      "Epoch 121/300\n",
      "75/75 [==============================] - 1s 17ms/step - loss: 0.1446 - accuracy: 0.9586 - val_loss: 0.2930 - val_accuracy: 0.9300\n",
      "Epoch 122/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1382 - accuracy: 0.9606 - val_loss: 0.2813 - val_accuracy: 0.9326\n",
      "Epoch 123/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1405 - accuracy: 0.9597 - val_loss: 0.2872 - val_accuracy: 0.9295\n",
      "Epoch 124/300\n",
      "75/75 [==============================] - 1s 17ms/step - loss: 0.1400 - accuracy: 0.9601 - val_loss: 0.2904 - val_accuracy: 0.9321\n",
      "Epoch 125/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1387 - accuracy: 0.9602 - val_loss: 0.2734 - val_accuracy: 0.9323\n",
      "Epoch 126/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1341 - accuracy: 0.9618 - val_loss: 0.2758 - val_accuracy: 0.9326\n",
      "Epoch 127/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1356 - accuracy: 0.9615 - val_loss: 0.2889 - val_accuracy: 0.9305\n",
      "Epoch 128/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1386 - accuracy: 0.9603 - val_loss: 0.2933 - val_accuracy: 0.9303\n",
      "Epoch 129/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1369 - accuracy: 0.9608 - val_loss: 0.2867 - val_accuracy: 0.9318\n",
      "Epoch 130/300\n",
      "75/75 [==============================] - 1s 17ms/step - loss: 0.1356 - accuracy: 0.9615 - val_loss: 0.2802 - val_accuracy: 0.9322\n",
      "Epoch 131/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1367 - accuracy: 0.9609 - val_loss: 0.2816 - val_accuracy: 0.9328\n",
      "Epoch 132/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1337 - accuracy: 0.9619 - val_loss: 0.3079 - val_accuracy: 0.9275\n",
      "Epoch 133/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1363 - accuracy: 0.9610 - val_loss: 0.2965 - val_accuracy: 0.9277\n",
      "Epoch 134/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1328 - accuracy: 0.9619 - val_loss: 0.2820 - val_accuracy: 0.9328\n",
      "Epoch 135/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1357 - accuracy: 0.9613 - val_loss: 0.2926 - val_accuracy: 0.9321\n",
      "Epoch 136/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1317 - accuracy: 0.9621 - val_loss: 0.2876 - val_accuracy: 0.9320\n",
      "Epoch 137/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1344 - accuracy: 0.9614 - val_loss: 0.2837 - val_accuracy: 0.9322\n",
      "Epoch 138/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1346 - accuracy: 0.9614 - val_loss: 0.2881 - val_accuracy: 0.9313\n",
      "Epoch 139/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1355 - accuracy: 0.9608 - val_loss: 0.2849 - val_accuracy: 0.9325\n",
      "Epoch 140/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1338 - accuracy: 0.9614 - val_loss: 0.2866 - val_accuracy: 0.9326\n",
      "Epoch 141/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1334 - accuracy: 0.9618 - val_loss: 0.2982 - val_accuracy: 0.9274\n",
      "Epoch 142/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1297 - accuracy: 0.9630 - val_loss: 0.2985 - val_accuracy: 0.9296\n",
      "Epoch 143/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1256 - accuracy: 0.9644 - val_loss: 0.2790 - val_accuracy: 0.9346\n",
      "Epoch 144/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1289 - accuracy: 0.9629 - val_loss: 0.2901 - val_accuracy: 0.9332\n",
      "Epoch 145/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1301 - accuracy: 0.9627 - val_loss: 0.2947 - val_accuracy: 0.9311\n",
      "Epoch 146/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1309 - accuracy: 0.9625 - val_loss: 0.3044 - val_accuracy: 0.9293\n",
      "Epoch 147/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1308 - accuracy: 0.9622 - val_loss: 0.2866 - val_accuracy: 0.9323\n",
      "Epoch 148/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1274 - accuracy: 0.9635 - val_loss: 0.2898 - val_accuracy: 0.9334\n",
      "Epoch 149/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1218 - accuracy: 0.9653 - val_loss: 0.2806 - val_accuracy: 0.9348\n",
      "Epoch 150/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1251 - accuracy: 0.9644 - val_loss: 0.2916 - val_accuracy: 0.9316\n",
      "Epoch 151/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1279 - accuracy: 0.9633 - val_loss: 0.3218 - val_accuracy: 0.9285\n",
      "Epoch 152/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1304 - accuracy: 0.9630 - val_loss: 0.2998 - val_accuracy: 0.9310\n",
      "Epoch 153/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1258 - accuracy: 0.9638 - val_loss: 0.2993 - val_accuracy: 0.9272\n",
      "Epoch 154/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1245 - accuracy: 0.9643 - val_loss: 0.2845 - val_accuracy: 0.9347\n",
      "Epoch 155/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1240 - accuracy: 0.9646 - val_loss: 0.2927 - val_accuracy: 0.9336\n",
      "Epoch 156/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1228 - accuracy: 0.9647 - val_loss: 0.2858 - val_accuracy: 0.9329\n",
      "Epoch 157/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1221 - accuracy: 0.9650 - val_loss: 0.2865 - val_accuracy: 0.9343\n",
      "Epoch 158/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1320 - accuracy: 0.9619 - val_loss: 0.2963 - val_accuracy: 0.9316\n",
      "Epoch 159/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1252 - accuracy: 0.9638 - val_loss: 0.3042 - val_accuracy: 0.9312\n",
      "Epoch 160/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1262 - accuracy: 0.9638 - val_loss: 0.3105 - val_accuracy: 0.9278\n",
      "Epoch 161/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1223 - accuracy: 0.9650 - val_loss: 0.2921 - val_accuracy: 0.9316\n",
      "Epoch 162/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1227 - accuracy: 0.9647 - val_loss: 0.2803 - val_accuracy: 0.9352\n",
      "Epoch 163/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1189 - accuracy: 0.9661 - val_loss: 0.2904 - val_accuracy: 0.9327\n",
      "Epoch 164/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1169 - accuracy: 0.9668 - val_loss: 0.3054 - val_accuracy: 0.9318\n",
      "Epoch 165/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1186 - accuracy: 0.9661 - val_loss: 0.3020 - val_accuracy: 0.9329\n",
      "Epoch 166/300\n",
      "75/75 [==============================] - 1s 17ms/step - loss: 0.1231 - accuracy: 0.9646 - val_loss: 0.2998 - val_accuracy: 0.9313\n",
      "Epoch 167/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1248 - accuracy: 0.9641 - val_loss: 0.2950 - val_accuracy: 0.9333\n",
      "Epoch 168/300\n",
      "75/75 [==============================] - 1s 17ms/step - loss: 0.1236 - accuracy: 0.9647 - val_loss: 0.2797 - val_accuracy: 0.9361\n",
      "Epoch 169/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1217 - accuracy: 0.9651 - val_loss: 0.3057 - val_accuracy: 0.9332\n",
      "Epoch 170/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1154 - accuracy: 0.9673 - val_loss: 0.2748 - val_accuracy: 0.9363\n",
      "Epoch 171/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1150 - accuracy: 0.9671 - val_loss: 0.2917 - val_accuracy: 0.9355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1150 - accuracy: 0.9673 - val_loss: 0.2899 - val_accuracy: 0.9352\n",
      "Epoch 173/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1189 - accuracy: 0.9662 - val_loss: 0.2991 - val_accuracy: 0.9329\n",
      "Epoch 174/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1204 - accuracy: 0.9653 - val_loss: 0.2963 - val_accuracy: 0.9338\n",
      "Epoch 175/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1191 - accuracy: 0.9656 - val_loss: 0.3009 - val_accuracy: 0.9343\n",
      "Epoch 176/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1165 - accuracy: 0.9666 - val_loss: 0.2902 - val_accuracy: 0.9345\n",
      "Epoch 177/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1150 - accuracy: 0.9672 - val_loss: 0.2872 - val_accuracy: 0.9361\n",
      "Epoch 178/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1161 - accuracy: 0.9669 - val_loss: 0.2981 - val_accuracy: 0.9312\n",
      "Epoch 179/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1208 - accuracy: 0.9653 - val_loss: 0.3123 - val_accuracy: 0.9284\n",
      "Epoch 180/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1190 - accuracy: 0.9657 - val_loss: 0.3081 - val_accuracy: 0.9314\n",
      "Epoch 181/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1199 - accuracy: 0.9655 - val_loss: 0.2977 - val_accuracy: 0.9342\n",
      "Epoch 182/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1160 - accuracy: 0.9664 - val_loss: 0.3035 - val_accuracy: 0.9321\n",
      "Epoch 183/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1141 - accuracy: 0.9672 - val_loss: 0.2818 - val_accuracy: 0.9369\n",
      "Epoch 184/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1166 - accuracy: 0.9665 - val_loss: 0.2979 - val_accuracy: 0.9318\n",
      "Epoch 185/300\n",
      "75/75 [==============================] - 1s 17ms/step - loss: 0.1162 - accuracy: 0.9664 - val_loss: 0.2933 - val_accuracy: 0.9352\n",
      "Epoch 186/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1114 - accuracy: 0.9678 - val_loss: 0.3274 - val_accuracy: 0.9244\n",
      "Epoch 187/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1116 - accuracy: 0.9678 - val_loss: 0.2894 - val_accuracy: 0.9362\n",
      "Epoch 188/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1119 - accuracy: 0.9682 - val_loss: 0.2968 - val_accuracy: 0.9319\n",
      "Epoch 189/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1120 - accuracy: 0.9679 - val_loss: 0.2856 - val_accuracy: 0.9365\n",
      "Epoch 190/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1135 - accuracy: 0.9676 - val_loss: 0.3068 - val_accuracy: 0.9331\n",
      "Epoch 191/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1167 - accuracy: 0.9661 - val_loss: 0.3304 - val_accuracy: 0.9246\n",
      "Epoch 192/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1165 - accuracy: 0.9664 - val_loss: 0.3085 - val_accuracy: 0.9308\n",
      "Epoch 193/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1122 - accuracy: 0.9677 - val_loss: 0.3128 - val_accuracy: 0.9345\n",
      "Epoch 194/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1131 - accuracy: 0.9676 - val_loss: 0.2895 - val_accuracy: 0.9348\n",
      "Epoch 195/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1080 - accuracy: 0.9688 - val_loss: 0.2967 - val_accuracy: 0.9370\n",
      "Epoch 196/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1077 - accuracy: 0.9691 - val_loss: 0.3037 - val_accuracy: 0.9353\n",
      "Epoch 197/300\n",
      "75/75 [==============================] - 1s 17ms/step - loss: 0.1142 - accuracy: 0.9672 - val_loss: 0.3281 - val_accuracy: 0.9228\n",
      "Epoch 198/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1089 - accuracy: 0.9687 - val_loss: 0.2759 - val_accuracy: 0.9377\n",
      "Epoch 199/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1103 - accuracy: 0.9685 - val_loss: 0.3033 - val_accuracy: 0.9347\n",
      "Epoch 200/300\n",
      "75/75 [==============================] - 1s 17ms/step - loss: 0.1103 - accuracy: 0.9683 - val_loss: 0.3016 - val_accuracy: 0.9347\n",
      "Epoch 201/300\n",
      "75/75 [==============================] - 1s 17ms/step - loss: 0.1121 - accuracy: 0.9678 - val_loss: 0.2988 - val_accuracy: 0.9359\n",
      "Epoch 202/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1103 - accuracy: 0.9683 - val_loss: 0.2891 - val_accuracy: 0.9359\n",
      "Epoch 203/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1098 - accuracy: 0.9683 - val_loss: 0.2915 - val_accuracy: 0.9354\n",
      "Epoch 204/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1127 - accuracy: 0.9676 - val_loss: 0.3080 - val_accuracy: 0.9332\n",
      "Epoch 205/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1094 - accuracy: 0.9686 - val_loss: 0.2862 - val_accuracy: 0.9378\n",
      "Epoch 206/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1084 - accuracy: 0.9687 - val_loss: 0.2964 - val_accuracy: 0.9352\n",
      "Epoch 207/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1077 - accuracy: 0.9690 - val_loss: 0.2897 - val_accuracy: 0.9357\n",
      "Epoch 208/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1075 - accuracy: 0.9688 - val_loss: 0.2947 - val_accuracy: 0.9355\n",
      "Epoch 209/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1061 - accuracy: 0.9695 - val_loss: 0.2982 - val_accuracy: 0.9358\n",
      "Epoch 210/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1079 - accuracy: 0.9689 - val_loss: 0.2927 - val_accuracy: 0.9357\n",
      "Epoch 211/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1089 - accuracy: 0.9688 - val_loss: 0.3004 - val_accuracy: 0.9348\n",
      "Epoch 212/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1073 - accuracy: 0.9690 - val_loss: 0.2951 - val_accuracy: 0.9377\n",
      "Epoch 213/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1057 - accuracy: 0.9696 - val_loss: 0.2932 - val_accuracy: 0.9367\n",
      "Epoch 214/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1046 - accuracy: 0.9697 - val_loss: 0.2904 - val_accuracy: 0.9365\n",
      "Epoch 215/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1044 - accuracy: 0.9701 - val_loss: 0.3047 - val_accuracy: 0.9351\n",
      "Epoch 216/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1071 - accuracy: 0.9691 - val_loss: 0.2989 - val_accuracy: 0.9354\n",
      "Epoch 217/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1076 - accuracy: 0.9688 - val_loss: 0.2910 - val_accuracy: 0.9379\n",
      "Epoch 218/300\n",
      "75/75 [==============================] - 1s 17ms/step - loss: 0.1060 - accuracy: 0.9694 - val_loss: 0.2964 - val_accuracy: 0.9352\n",
      "Epoch 219/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1039 - accuracy: 0.9701 - val_loss: 0.3046 - val_accuracy: 0.9338\n",
      "Epoch 220/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1045 - accuracy: 0.9698 - val_loss: 0.2919 - val_accuracy: 0.9371\n",
      "Epoch 221/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1062 - accuracy: 0.9692 - val_loss: 0.3390 - val_accuracy: 0.9283\n",
      "Epoch 222/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1099 - accuracy: 0.9681 - val_loss: 0.3071 - val_accuracy: 0.9326\n",
      "Epoch 223/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1052 - accuracy: 0.9696 - val_loss: 0.2981 - val_accuracy: 0.9353\n",
      "Epoch 224/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1026 - accuracy: 0.9702 - val_loss: 0.3004 - val_accuracy: 0.9351\n",
      "Epoch 225/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1022 - accuracy: 0.9705 - val_loss: 0.3042 - val_accuracy: 0.9362\n",
      "Epoch 226/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1013 - accuracy: 0.9706 - val_loss: 0.2925 - val_accuracy: 0.9376\n",
      "Epoch 227/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1143 - accuracy: 0.9671 - val_loss: 0.3335 - val_accuracy: 0.9293\n",
      "Epoch 228/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1167 - accuracy: 0.9664 - val_loss: 0.3215 - val_accuracy: 0.9274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 229/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1133 - accuracy: 0.9670 - val_loss: 0.3077 - val_accuracy: 0.9330\n",
      "Epoch 230/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1031 - accuracy: 0.9701 - val_loss: 0.2930 - val_accuracy: 0.9379\n",
      "Epoch 231/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.0985 - accuracy: 0.9717 - val_loss: 0.3053 - val_accuracy: 0.9338\n",
      "Epoch 232/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.0977 - accuracy: 0.9715 - val_loss: 0.3042 - val_accuracy: 0.9333\n",
      "Epoch 233/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.0981 - accuracy: 0.9717 - val_loss: 0.2963 - val_accuracy: 0.9372\n",
      "Epoch 234/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.0980 - accuracy: 0.9717 - val_loss: 0.2923 - val_accuracy: 0.9381\n",
      "Epoch 235/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.0976 - accuracy: 0.9717 - val_loss: 0.3060 - val_accuracy: 0.9351\n",
      "Epoch 236/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.0990 - accuracy: 0.9714 - val_loss: 0.3073 - val_accuracy: 0.9360\n",
      "Epoch 237/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1010 - accuracy: 0.9708 - val_loss: 0.3174 - val_accuracy: 0.9310\n",
      "Epoch 238/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1020 - accuracy: 0.9704 - val_loss: 0.3275 - val_accuracy: 0.9336\n",
      "Epoch 239/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1042 - accuracy: 0.9694 - val_loss: 0.3142 - val_accuracy: 0.9324\n",
      "Epoch 240/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1014 - accuracy: 0.9705 - val_loss: 0.2990 - val_accuracy: 0.9366\n",
      "Epoch 241/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1036 - accuracy: 0.9700 - val_loss: 0.3058 - val_accuracy: 0.9340\n",
      "Epoch 242/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1049 - accuracy: 0.9695 - val_loss: 0.3261 - val_accuracy: 0.9301\n",
      "Epoch 243/300\n",
      "75/75 [==============================] - 1s 17ms/step - loss: 0.1035 - accuracy: 0.9700 - val_loss: 0.3000 - val_accuracy: 0.9352\n",
      "Epoch 244/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.0964 - accuracy: 0.9719 - val_loss: 0.2992 - val_accuracy: 0.9362\n",
      "Epoch 245/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.0993 - accuracy: 0.9712 - val_loss: 0.2977 - val_accuracy: 0.9382\n",
      "Epoch 246/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.0958 - accuracy: 0.9723 - val_loss: 0.2998 - val_accuracy: 0.9368\n",
      "Epoch 247/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.0977 - accuracy: 0.9716 - val_loss: 0.2889 - val_accuracy: 0.9392\n",
      "Epoch 248/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.0996 - accuracy: 0.9711 - val_loss: 0.3049 - val_accuracy: 0.9353\n",
      "Epoch 249/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1006 - accuracy: 0.9706 - val_loss: 0.3234 - val_accuracy: 0.9302\n",
      "Epoch 250/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1029 - accuracy: 0.9701 - val_loss: 0.3054 - val_accuracy: 0.9363\n",
      "Epoch 251/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1012 - accuracy: 0.9705 - val_loss: 0.3044 - val_accuracy: 0.9359\n",
      "Epoch 252/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1008 - accuracy: 0.9706 - val_loss: 0.2996 - val_accuracy: 0.9354\n",
      "Epoch 253/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.0950 - accuracy: 0.9726 - val_loss: 0.2990 - val_accuracy: 0.9355\n",
      "Epoch 254/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.0943 - accuracy: 0.9727 - val_loss: 0.3232 - val_accuracy: 0.9309\n",
      "Epoch 255/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.0950 - accuracy: 0.9727 - val_loss: 0.2972 - val_accuracy: 0.9385\n",
      "Epoch 256/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.0976 - accuracy: 0.9716 - val_loss: 0.3228 - val_accuracy: 0.9357\n",
      "Epoch 257/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.0984 - accuracy: 0.9715 - val_loss: 0.3177 - val_accuracy: 0.9349\n",
      "Epoch 258/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.0980 - accuracy: 0.9715 - val_loss: 0.3187 - val_accuracy: 0.9338\n",
      "Epoch 259/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.0976 - accuracy: 0.9717 - val_loss: 0.3311 - val_accuracy: 0.9300\n",
      "Epoch 260/300\n",
      "75/75 [==============================] - 1s 17ms/step - loss: 0.0993 - accuracy: 0.9711 - val_loss: 0.3222 - val_accuracy: 0.9329\n",
      "Epoch 261/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.0976 - accuracy: 0.9715 - val_loss: 0.3045 - val_accuracy: 0.9374\n",
      "Epoch 262/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.0957 - accuracy: 0.9723 - val_loss: 0.3019 - val_accuracy: 0.9382\n",
      "Epoch 263/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.0934 - accuracy: 0.9728 - val_loss: 0.3049 - val_accuracy: 0.9384\n",
      "Epoch 264/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.0930 - accuracy: 0.9731 - val_loss: 0.3070 - val_accuracy: 0.9372\n",
      "Epoch 265/300\n",
      "75/75 [==============================] - 1s 17ms/step - loss: 0.0930 - accuracy: 0.9729 - val_loss: 0.3084 - val_accuracy: 0.9369\n",
      "Epoch 266/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.0917 - accuracy: 0.9736 - val_loss: 0.3178 - val_accuracy: 0.9344\n",
      "Epoch 267/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.0993 - accuracy: 0.9710 - val_loss: 0.3586 - val_accuracy: 0.9207\n",
      "Epoch 268/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.0996 - accuracy: 0.9709 - val_loss: 0.3359 - val_accuracy: 0.9289\n",
      "Epoch 269/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.0988 - accuracy: 0.9712 - val_loss: 0.3043 - val_accuracy: 0.9363\n",
      "Epoch 270/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.0946 - accuracy: 0.9724 - val_loss: 0.3113 - val_accuracy: 0.9359\n",
      "Epoch 271/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.0976 - accuracy: 0.9715 - val_loss: 0.3069 - val_accuracy: 0.9355\n",
      "Epoch 272/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.0930 - accuracy: 0.9726 - val_loss: 0.2969 - val_accuracy: 0.9366\n",
      "Epoch 273/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.0913 - accuracy: 0.9735 - val_loss: 0.2948 - val_accuracy: 0.9390\n",
      "Epoch 274/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.0900 - accuracy: 0.9739 - val_loss: 0.3087 - val_accuracy: 0.9387\n",
      "Epoch 275/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.0915 - accuracy: 0.9734 - val_loss: 0.3105 - val_accuracy: 0.9379\n",
      "Epoch 276/300\n",
      "75/75 [==============================] - 1s 17ms/step - loss: 0.0923 - accuracy: 0.9731 - val_loss: 0.3102 - val_accuracy: 0.9366\n",
      "Epoch 277/300\n",
      "75/75 [==============================] - 1s 17ms/step - loss: 0.0908 - accuracy: 0.9737 - val_loss: 0.3067 - val_accuracy: 0.9384\n",
      "Epoch 278/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.0955 - accuracy: 0.9724 - val_loss: 0.3322 - val_accuracy: 0.9292\n",
      "Epoch 279/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.0909 - accuracy: 0.9734 - val_loss: 0.3061 - val_accuracy: 0.9386\n",
      "Epoch 280/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.0963 - accuracy: 0.9716 - val_loss: 0.3237 - val_accuracy: 0.9339\n",
      "Epoch 281/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.0949 - accuracy: 0.9722 - val_loss: 0.3156 - val_accuracy: 0.9367\n",
      "Epoch 282/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.1013 - accuracy: 0.9702 - val_loss: 0.3644 - val_accuracy: 0.9294\n",
      "Epoch 283/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.0979 - accuracy: 0.9712 - val_loss: 0.3167 - val_accuracy: 0.9344\n",
      "Epoch 284/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.0946 - accuracy: 0.9721 - val_loss: 0.3011 - val_accuracy: 0.9376\n",
      "Epoch 285/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.0900 - accuracy: 0.9739 - val_loss: 0.3217 - val_accuracy: 0.9327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 286/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.0885 - accuracy: 0.9742 - val_loss: 0.3027 - val_accuracy: 0.9395\n",
      "Epoch 287/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.0860 - accuracy: 0.9751 - val_loss: 0.2934 - val_accuracy: 0.9408\n",
      "Epoch 288/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.0854 - accuracy: 0.9756 - val_loss: 0.3097 - val_accuracy: 0.9387\n",
      "Epoch 289/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.0895 - accuracy: 0.9740 - val_loss: 0.3186 - val_accuracy: 0.9350\n",
      "Epoch 290/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.0912 - accuracy: 0.9732 - val_loss: 0.3192 - val_accuracy: 0.9373\n",
      "Epoch 291/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.0936 - accuracy: 0.9726 - val_loss: 0.3616 - val_accuracy: 0.9293\n",
      "Epoch 292/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.0990 - accuracy: 0.9707 - val_loss: 0.3384 - val_accuracy: 0.9309\n",
      "Epoch 293/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.0946 - accuracy: 0.9720 - val_loss: 0.3209 - val_accuracy: 0.9347\n",
      "Epoch 294/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.0945 - accuracy: 0.9721 - val_loss: 0.3294 - val_accuracy: 0.9326\n",
      "Epoch 295/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.0891 - accuracy: 0.9739 - val_loss: 0.3236 - val_accuracy: 0.9353\n",
      "Epoch 296/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.0857 - accuracy: 0.9750 - val_loss: 0.3032 - val_accuracy: 0.9400\n",
      "Epoch 297/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.0852 - accuracy: 0.9751 - val_loss: 0.3142 - val_accuracy: 0.9396\n",
      "Epoch 298/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.0869 - accuracy: 0.9748 - val_loss: 0.3157 - val_accuracy: 0.9349\n",
      "Epoch 299/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.0895 - accuracy: 0.9740 - val_loss: 0.3240 - val_accuracy: 0.9348\n",
      "Epoch 300/300\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.0919 - accuracy: 0.9732 - val_loss: 0.3344 - val_accuracy: 0.9329\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=4056, epochs=300, validation_data = (X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffed59b1",
   "metadata": {},
   "source": [
    "# 4.Evaluate Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28731190",
   "metadata": {},
   "source": [
    "## 4.1. Plotting Loss and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "971e6040",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAybUlEQVR4nO3dd3wc5Z348c93u7SrZhUXyeCKKy5YmB7scHQIgcAdHAkl4QiEg4NLCByQhORHLnAFcoQEQhJCKqEbCCRUG5tqy8YV29i4Sm4qVi/bnt8fz8qWbZWVLHm1u9/366XXrmZmZ57Zmf3Od5555hkxxqCUUir5ORJdAKWUUv1DA7pSSqUIDehKKZUiNKArpVSK0ICulFIpwpWoBRcUFJhRo0YlavFKKZWUli5dWmWMKexsXMIC+qhRoygrK0vU4pVSKimJyNauxmmVi1JKpYgeA7qI+ERksYisEJE1IvLDTqbxisjTIrJRRD4WkVEDUlqllFJdiidDbwO+aIyZDswAzhGREw+a5hvAXmPMOOAh4IF+LaVSSqke9ViHbmzfAI2xf92xv4P7C7gIuDf2/jngERERo/0KKJU0QqEQ5eXltLa2JrooCvD5fJSUlOB2u+P+TFwXRUXECSwFxgE/N8Z8fNAkxcB2AGNMWETqgHygKu6SKKUSqry8nKysLEaNGoWIJLo4ac0YQ3V1NeXl5YwePTruz8V1UdQYEzHGzABKgNkiMrUvhRSR60WkTETKKisr+zILpdQAaW1tJT8/X4P5ICAi5Ofn9/psqVetXIwxtcB84JyDRlUAI2MFcQE5QHUnn3/cGFNqjCktLOy0GaVSKoE0mA8efdkW8bRyKRSR3Nj7DOBMYN1Bk70MXB17fynwzoDVn+9eA2//CJprBmT2SimVrOLJ0IcD80VkJbAEeNMY81cR+ZGIfCk2zW+AfBHZCPw7cOfAFBeo2QSL/hdqtw3YIpRSR151dTUzZsxgxowZDBs2jOLi4n3/B4PBbj9bVlbGLbfc0uMyTj755H4p64IFC7jgggv6ZV79KZ5WLiuBmZ0M/36H963AZf1btC4EhtrXJq2DVyqV5Ofns3z5cgDuvfdeAoEA3/nOd/aND4fDuFydh6zS0lJKS0t7XMYHH3zQL2UdrJLvTlF/rO69cXdiy6GUGnDXXHMNN9xwAyeccALf/e53Wbx4MSeddBIzZ87k5JNPZv369cCBGfO9997L17/+debMmcOYMWN4+OGH980vEAjsm37OnDlceumlTJw4kSuvvJL2WuLXXnuNiRMnMmvWLG655ZZeZeJPPfUUxx57LFOnTuWOO+4AIBKJcM011zB16lSOPfZYHnroIQAefvhhJk+ezLRp07j88ssP/8sigX259FmgyL5qQFdqwPzwlTV8uqO+X+c5eUQ2P7hwSq8/V15ezgcffIDT6aS+vp5Fixbhcrl46623uOuuu3j++ecP+cy6deuYP38+DQ0NTJgwgRtvvPGQ9tyffPIJa9asYcSIEZxyyim8//77lJaW8s1vfpOFCxcyevRorrjiirjLuWPHDu644w6WLl1KXl4eZ511FvPmzWPkyJFUVFSwevVqAGprawG4//772bx5M16vd9+ww5V8GbrHD54saNQqF6XSwWWXXYbT6QSgrq6Oyy67jKlTp3LbbbexZs2aTj9z/vnn4/V6KSgooKioiN27D00AZ8+eTUlJCQ6HgxkzZrBlyxbWrVvHmDFj9rX97k1AX7JkCXPmzKGwsBCXy8WVV17JwoULGTNmDJs2beLmm2/m73//O9nZ2QBMmzaNK6+8kj/+8Y9dViX1VvJl6ACBQs3QlRpAfcmkB4rf79/3/nvf+x5z587lxRdfZMuWLcyZM6fTz3i93n3vnU4n4XC4T9P0h7y8PFasWMHrr7/OY489xjPPPMMTTzzBq6++ysKFC3nllVf48Y9/zKpVqw47sCdfhg72wqheFFUq7dTV1VFcXAzAk08+2e/znzBhAps2bWLLli0APP3003F/dvbs2bz77rtUVVURiUR46qmnOP3006mqqiIajfKVr3yF++67j2XLlhGNRtm+fTtz587lgQceoK6ujsbGxp4X0oPkzND9hVB5cFN4pVSq++53v8vVV1/Nfffdx/nnn9/v88/IyOAXv/gF55xzDn6/n+OPP77Lad9++21KSkr2/f/ss89y//33M3fuXIwxnH/++Vx00UWsWLGCa6+9lmg0CsBPfvITIpEIX/3qV6mrq8MYwy233EJubu5hl18S1X9WaWmp6fMDLl79Dqx6Fu7ssp93pVQvrV27lkmTJiW6GAnX2NhIIBDAGMNNN93E+PHjue222xJSls62iYgsNcZ02kYzSatciqC1FsJtiS6JUirF/OpXv2LGjBlMmTKFuro6vvnNbya6SHFLziqX9qaLTZWQU9L9tEop1Qu33XZbwjLyw5WcGbpf26IrpdTBkjOgt9/+r23RlVJqnyQN6JqhK6XUwZIzoLf359K0J7HlUEqpQSQ5L4q6feDLgUYN6Eqliurqas444wwAdu3ahdPppP1BOIsXL8bj8XT7+QULFuDxeDrtIvfJJ5+krKyMRx55pP8LPogkZ0AHe2FUq1yUShk9dZ/bkwULFhAIBPqtz/NklJxVLmAvjOpFUaVS2tKlSzn99NOZNWsWZ599Njt37gQO7Xp2y5YtPPbYYzz00EPMmDGDRYsWxTX/Bx98kKlTpzJ16lR++tOfAtDU1MT555/P9OnTmTp16r7b/++88859y+zNgeZISt4MPVAIO1cmuhRKpaa/3Qm7VvXvPIcdC+feH/fkxhhuvvlmXnrpJQoLC3n66ae5++67eeKJJw7pejY3N5cbbrihV1n90qVL+e1vf8vHH3+MMYYTTjiB008/nU2bNjFixAheffVVwPYfU11dzYsvvsi6desQkX7r7ra/JXeGrh10KZWy2traWL16NWeeeSYzZszgvvvuo7y8HOifrmffe+89Lr74Yvx+P4FAgEsuuYRFixZx7LHH8uabb3LHHXewaNEicnJyyMnJwefz8Y1vfIMXXniBzMzM/lzVfpPEGXoRtNVDqAXcGYkujVKppReZ9EAxxjBlyhQ+/PDDQ8Z11vVsfznmmGNYtmwZr732Gvfccw9nnHEG3//+91m8eDFvv/02zz33HI888gjvvPNOvy2zvyRvhr7vblFt6aJUKvJ6vVRWVu4L6KFQiDVr1nTZ9WxWVhYNDQ1xz/+0005j3rx5NDc309TUxIsvvshpp53Gjh07yMzM5Ktf/Sq33347y5Yto7Gxkbq6Os477zweeughVqxYMVCrfViSOENvv1t0D+QdndiyKKX6ncPh4LnnnuOWW26hrq6OcDjMrbfeyjHHHNNp17MXXnghl156KS+99BI/+9nPOO200w6Y35NPPsm8efP2/f/RRx9xzTXXMHv2bACuu+46Zs6cyeuvv87tt9+Ow+HA7Xbz6KOP0tDQwEUXXURrayvGGB588MEj+VXELTm7zwXY8Qk8Pgcu/zNM7P9+kZVKN9p97uCTHt3nQocMXduiK6UUJHNAb7/9X9uiK6UUkMwB3emGjCGaoSvVjxJVBasO1ZdtkbwBHWJ3i2pAV6o/+Hw+qqurNagPAsYYqqur8fl8vfpcj61cRGQk8HtgKGCAx40x/3fQNHOAl4DNsUEvGGN+1KuS9EWgUG8uUqqflJSUUF5eTmWl/qYGA5/Pd8BDqOMRT7PFMPBtY8wyEckClorIm8aYTw+abpEx5oJeLf1wBYZC+ZIjukilUpXb7Wb06NGJLoY6DD1WuRhjdhpjlsXeNwBrgeKBLlhc/EV6UVQppWJ6VYcuIqOAmcDHnYw+SURWiMjfRGRKF5+/XkTKRKSsX07rAkUQaoK2xsOfl1JKJbm4A7qIBIDngVuNMfUHjV4GHG2MmQ78DJjX2TyMMY8bY0qNMaXtHdcflvZH0emTi5RSKr6ALiJubDD/kzHmhYPHG2PqjTGNsfevAW4RKejXknYma5h9rd8x4ItSSqnBrseALiIC/AZYa4zptAMDERkWmw4RmR2bb3V/FrRT+ePsa9WGAV+UUkoNdvG0cjkF+BqwSkSWx4bdBRwFYIx5DLgUuFFEwkALcLk5Eo1Zs0vAnQlVnw34opRSarDrMaAbY94DpIdpHgGO/NNXHQ6bpWtAV0qpJL9TFKBwAlRqQFdKqeQP6AUToG4bBJsSXRKllEqoFAjo4+2rXhhVSqW55A/ow6fZ153LE1oMpZRKtOQP6HmjITNf+3RRSqW95A/oIlBcCuWH8Tg7pZRKAckf0AFKSqFyPbTWJbokSimVMKkT0DFQsSzRJVFKqYRJjYA+4jj7qtUuSqk0lhoBPSPXtkev0ICulEpfqRHQAUqOty1d9HmISqk0lUIBfRY0V8PeLYkuiVJKJUQKBfTj7avWoyul0lTqBPTCSbYrXb3BSCmVplInoDtdtrWLXhhVSqWp1AnoYNuj71wJodZEl0QppY641Avo0RDsWpnokiil1BGXWgE9P9aVbu22xJZDKaUSILUCur/AvjYP/POplVJqsEmtgJ6RB4gGdKVUWkqtgO5w2qDeVJXokiil1BGXWgEd7MMuNENXSqWh1Avo/gIN6EqptJR6AT0zX6tclFJpKTUDumboSqk01GNAF5GRIjJfRD4VkTUi8m+dTCMi8rCIbBSRlSJy3MAUNw7tAT0aTVgRlFIqEeLJ0MPAt40xk4ETgZtEZPJB05wLjI/9XQ882q+l7A1/AZgItOnzRZVS6aXHgG6M2WmMWRZ73wCsBYoPmuwi4PfG+gjIFZHh/V7aeGTGbi5q0moXpVR66VUduoiMAmYCHx80qhjY3uH/cg4N+kdGZr591Xp0pVSaiTugi0gAeB641RhT35eFicj1IlImImWVlZV9mUXPMofYVw3oSqk0E1dAFxE3Npj/yRjzQieTVAAjO/xfEht2AGPM48aYUmNMaWFhYV/K2zOP376Gmgdm/kopNUjF08pFgN8Aa40xD3Yx2cvAVbHWLicCdcaYnf1Yzvi5vPY1rH2iK6XSiyuOaU4BvgasEpHlsWF3AUcBGGMeA14DzgM2As3Atf1e0ni5MuyrBnSlVJrpMaAbY94DpIdpDHBTfxXqsLh99lWfWqSUSjOpd6eoKxbQwy2JLYdSSh1hqRfQnR5AINyW6JIopdQRlXoBXQTcGRDSDF0plV5SL6CDbemiF0WVUmkmRQN6hgZ0pVTaSc2A7vZpKxelVNpJzYDu8mmGrpRKOxrQlVIqRaRuQNcqF6VUmknNgO7WDF0plX5SM6BrKxelVBpK0YDu1RuLlFJpJzUDujtDb/1XSqWd1AzoLp92zqWUSjspHNA1Q1dKpZfUDOhun9ahK6XSTtIF9AXr93DWQ++yrbqbZ4a6MsBEIBI6cgVTSqkES7qA3hqK8NnuRhrbwl1PpM8VVUqloaQL6F6XE4BgJNr1RO7Yc0X1blGlVBpJuoDucdkit4UiXU+0L0PXenSlVPpIuoDujQX0bjN0VyxD15YuSqk0knQBfX+G3l2VS+xB0drSRSmVRpIuoMdVh+6KBXTN0JVSaSTpAvq+DD3cXR16e0DXDF0plT6SLqB746py0VYuSqn0k3QB3RPXRVFt5aKUSj89BnQReUJE9ojI6i7GzxGROhFZHvv7fv8Xc7+4MvR9VS7BgSyKUkoNKq44pnkSeAT4fTfTLDLGXNAvJepBXBdFnR77GtGLokqp9NFjhm6MWQjUHIGyxMXtFKCnG4vaM3StQ1dKpY/+qkM/SURWiMjfRGRKVxOJyPUiUiYiZZWVlX1akIjgdTlo67YOPZaha5WLUiqN9EdAXwYcbYyZDvwMmNfVhMaYx40xpcaY0sLCwj4v0ONyxFmHrhm6Uip9HHZAN8bUG2MaY+9fA9wiUnDYJeuG1+XsoQ491solohm6Uip9HHZAF5FhIiKx97Nj86w+3Pl2x9tThu5wgMOtd4oqpdJKj61cROQpYA5QICLlwA8AN4Ax5jHgUuBGEQkDLcDlxhgzYCXGBvRuM3SwbdE1oCul0kiPAd0Yc0UP4x/BNms8YmwdejetXMA2XdRmi0qpNJJ0d4pCvBm6Ty+KKqXSSlIG9B5buYBtuqjNFpVSaSQpA3qPrVxAM3SlVNpJ0oDu6L77XIjVoWuGrpRKH0kZ0D0uB8FwPBm6XhRVSqWPpAzoNkPXZotKKdVRUgb0+DJ0rzZbVEqllaQM6F6Xs+cM3akZulIqvSRlQI8vQ/doQFdKpZWkDOhxtXLRi6JKqTSTlAHd43IQihii0W66jNFb/5VSaSYpA3pcj6HTDF0plWaSNKDH86BorUNXSqWXpAzonvaAHunhuaKRNhjYnnyVUmrQSMqAHleG7vSCiUI0fIRKpZRSiZWUAb09Q+++Dr39QdFa7aKUSg9JGdDbL4q2dveQi30PitaArpRKD0kZ0ANe+6Cl5mA3Ad0Zy9C16aJSKk0kZUD3e22G3tjaTf24ZuhKqTSTlAE9y2cz9Ma27gK61qErpdJLUgZ0vzeegB7L0LXKRSmVJpIyoLfXoTd1F9CdXvuqGbpSKk0kZUD3e2xAb+i2Dl2rXJRS6SUpA7rDIfg9Tq1yUUqpDpIyoAMEfK4eqlw0Q1dKpZekDeh+r4uGeDJ0DehKqTTRY0AXkSdEZI+IrO5ivIjIwyKyUURWishx/V/MQ2V5e8jQtQ5dKZVm4snQnwTO6Wb8ucD42N/1wKOHX6ye+b2u+G4s0jp0pVSa6DGgG2MWAjXdTHIR8HtjfQTkisjw/ipgVwJeV/cXRbXZolIqzfRHHXoxsL3D/+WxYYcQketFpExEyiorKw9roT0GdK1yUUqlmSN6UdQY87gxptQYU1pYWHhY8wr4egroWuWilEov/RHQK4CRHf4viQ0bUP7YRVHT1ROJHC5ANENXSqWN/gjoLwNXxVq7nAjUGWN29sN8uxXwughFDG3hLh5yIaIPilZKpRVXTxOIyFPAHKBARMqBHwBuAGPMY8BrwHnARqAZuHagCttRx/5cfG5n5xPpg6KVUmmkx4BujLmih/EGuKnfShSnQIceF/MD3s4nan9QtFJKpYGkvVM04Iujgy6nVzN0pVTaSNqAnpdpmyXubQ52PZFWuSil0kjSBvSCgA3oVY3dBGyXDyLdBHyllEohSRvQ2+vNqxq6CdhOD4Rbj1CJlFIqsZI2oGf7XHicjp4zdK1yUUqliaQN6CJCQcBDVaPWoSulFCRxQAcoyPLGUYeuAV0plR6SO6AHegjoTg+E9aKoUio9JHlA9/SQoXv1oqhSKm0kdUDPD3ipbgwSjXbRQZfLq80WlVJpI6kDekHASzhqqG8NdT6BUzN0pVT6SPKA3sPNRS6f1qErpdJGUgf04TkZAJTvbel8ApfeWKSUSh9JHdDHFQUA2LinsfMJXD6IhiDaRZ/pSimVQpI6oA/xexji9/B5ZRcB3Rl7rqheGFVKpYGkDugA4woDbNjdVYYe6yddq12UUmkg+QP60AAb9jR2/mzR9oCuGbpSKg0kf0AvDFDXEuq8TxenZuhKqfSR9AH9mKFZAKzdWX/oSJfPvmrTRaVUGkj6gD7jqFycDuGjTdWHjnTFLopqhq6USgNJH9ADXhfTS3L4sNOAHsvQtcdFpVQaSPqADnDy2AJWltfR2HbQA6Pbmy1qlYtSKg2kRkAfl08kanh3feWBI7TZolIqjaREQD9hdD7Dc3w8Xbb9wBHabFEplUZSIqA7HcJlpSNZtKGS8r3NHUZohq6USh9xBXQROUdE1ovIRhG5s5Px14hIpYgsj/1d1/9F7d4/lpYA8ExZ+f6B+5ot6kVRpVTq6zGgi4gT+DlwLjAZuEJEJncy6dPGmBmxv1/3czl7VJKXyWnjC3m2bDuR9gdeZA0FBPZuOdLFUUqpIy6eDH02sNEYs8kYEwT+Alw0sMXqmyuOH8nOulbeWrvbDvBmQf442LE8oeVSSqkjIZ6AXgx0vNpYHht2sK+IyEoReU5ERnY2IxG5XkTKRKSssrKys0kOy5mTh3J0fiY/e2fD/r5dRsyAncv7fVlKKTXY9NdF0VeAUcaYacCbwO86m8gY87gxptQYU1pYWNhPi97P5XRw09xxrK6o5++rd9mBw2dAfQU09v8BRCmlBpN4AnoF0DHjLokN28cYU22Mab/y+GtgVv8Ur/cumVnMpOHZ/PCVT+2NRiNm2BE7liWqSEopdUTEE9CXAONFZLSIeIDLgZc7TiAiwzv8+yVgbf8VsXdcTgc/vngqexpaufGPSwkWTQdfLnz4CLx2u9anK6VSVo8B3RgTBv4VeB0bqJ8xxqwRkR+JyJdik90iImtEZAVwC3DNQBU4Hscdlcf9l0xj0YYqfrqwAk69FTYvhMWPw58ug92fJrJ4Sik1IKTTB0McAaWlpaasrGxAl3H7syt44ZMKnrtuBjNX/ScUz4K3fwitdfDFe6B6E0y9GMb9w4EfbKwEbwDcGQNaPqWU6i0RWWqMKe10XCoH9NrmIBc+8h71LWHuOGcilxxXjC9YCy//K6x/zU7kCcA1f4XVz0PDLph9PTxzFYw9A7788wEtn1JK9VbaBnSA7TXNXPe7MtbvbmDisCyeuOZ4RvgFFvwnDJsGb9xjAzkG3H5w+6C5GtyZcNsa8OWAw9n7BQeb7JlA9oh+XyelVPpK64AOYIzhnXV7uPUvyxld6OfZG07C64oF6b1bYd6NMPEC22/6W/cCAhhwuGDMHPAXQv5Y+MLt8S/0L1dCxTL4909BpP9XSil1ZAWbwONPdCm6Degp0TlXT0SEMyYN5b8vm87K8jquePyj/Z145R0N174GJ30LZlxpg/gx50DJbMgfDxvfghVPwTv3wZp59jPv/RTe+iFs/QDKOzkobV8C6/4KDTugeuORWk2lVHc+nw9vfA/6ksRueQ/uPxp2re56mqZqePQU2PBW38t4mFwJW3ICnDN1GA9fMZO7X1zFP/3yIy6YPpxzpgxj5lF5doJAEVzxNOSPgSFj7LCPHgN/AXz0KDx/HdRsgnf+H5govPcgiBPO/BHMuho2vAkNO2H5n231TagJtn0EBePtvFpq7QGgZBZk5CXkO1DqsIRaoaIMRp2a6JL03jv32bIXjIfjrurdZ1c/D9EQfDoPhk3tfJo1L8Du1fDyzXDTx+DL7ny6+p329+/29a4McUiLDL2jL00fwVP/ciKhSJRfLdzEpY99yMNvb9jfodf4f9gfzAFOvAGOvRS++jyMnG1bybh8cM4DcPZ/wti58Mbd8JMSeO5aeP0u2L0GLnvSbrTtH0GwGZpr7MXYP30FfjbLtqQxBj5/Bxbcb38o7eLJIKJRewAJNvc8bW9VfgbVn/f/fA8WjQ78MvoiEran14OBMbDkNwduj2gU/nYHfPrSodOveBo2vj1w5Vn8ODx5vs1YO9q1Cn46DVY91/M8Wutg4f9AVYez17V/hcY9nU+/d6v9TEdNVXZYUxXUlUMkBH+/yyZddeWHzqP6cxvM3Zn2ullrJw+V72jrB/Db8+28tn0E6161w9fFGlO01B76mdXPg78IGnfB89+AV/4NPvwFhFrseGPsNnv0JJsUDoC0qEPvTDRqaAyG+d681by0fAczj8rl6pNGceKYfIbldHHkjITgg59BTglM+0c7zBjY/rFt5x4YascFm2Dyl+DP/wSbFgAC0bA9wk+7HFY9az8fCcHq2A9g3Jkw6UJY8BOIRuDLj0Jbva1/X/xr28yypNRW4ax+3p4hLPpfmHwRXPY7aGuwnZF1V18fjYA47DTGwFNX2LOS8/5n/wO1o1F4eLp9fN9NS8Dh2L+ef/su7FwBp94GR59iD2wuD3z8S1j7Clz53KFZR8tee1YydIqtspp9PQQb7M6+4U24fgEUTujbRuyoYbc9gLavx+F45mpb5hvf6/pMqmYz7N0MY7+4f5gxh37/0UjPF9XDbfDJH+0F9HFngrPDifP2xfCbM2Hyl+EfYz1qLP4VvPYdexZ46q22RVbJLBtUf/kFKJoMN75/6HKCTfZhL4dzdvjrM6F8MYw/C658dv/wv94GZU/Y9/9wL5xyq91H2/c3sAF428fw6r9D3XZ7beqql2w1xovX2+tVX5t34He4ew38+h8gbxR8/XWb9bY1wM9PsDcMRtrsb8KTZfcrp9eu37+8AznFNvDu3Qzv/rdt2Xb5n+EvV8Dx/wIjZtpl5hTbbfD5fGjcbevJ330Aqj6DjCHQUmPLMmwa7Fpp9//3/w8ueAiOuxo+fgwW/rdtTDH3Hvv51//DliXSZq/P5YyEbR/afqWGz4Cv/AYKxvVpE6T9RdHuGGN4ecUO7n15DXubQ4jA5ccfxQ8unIzP3YfWLR1tec/u5P4iG9hqNsHVL9vsatnv7M4+5z/sDvi3O8BEbFv5pkqo3bZ/PuKwP46DBYbaHbBkts0+pn7FnjUs+Im9IHvM2XaaaNgu44177Ou5D9gf91/+2c5nwnlw2rdt9tKwE/54iR1+5fP2jCUahbfvtTuxLxdaa+14Xy4cf509yEXaYM5dcNzXoH4HDJ9uy/zkBTYAZJdAfTmc/6ANUqufAwSO/4Ytz4Ebxf5wxGkPBAcHSWPsjzS7xAbwunJ4ZDYMGQ3/9EdbReZwHXgfgTGw8hnY8QlMPN9WGVQsgze/Z3+A//QnO6+dK+GXp9nPjJgJUy6BE26AcItt8bR3C8z/T5uJmghc8mt7Brfgfvv9FE6Ao06ygcJEbMZ43VuQe7TtU2jZ7+33XPO5PbgVTYJnr4XPY1n1sGPtgX7u3XbbrpkHW98DhxsueBByj4Kn/hmGT7M3yLXVQdYIOPOHtgw1sUx+0pegaoO9mD/+LJhwLjxxjg1clz4BwUYoOMYut/1spOoze0ApmmT32xV/sYnHpAth6ZP71zm72K7LMefYfbWx0u7fY+eC020TjmPOhU3z7b4782u219M37rYHlCFjYe5dtj472GgfQOPx23lMudjOv7UORp5gA2Ww0QZmE4GiKTaob/tw/7Y9/jq7r4z+gt0HfnO23Wc8frs/g90f5t5l9/PffQk2vxsb7oaTb7ZJ2daDDoIjT7DDp15qP3/6d+F3F9p1d7jsuo0/y14vGzPXJjmzr7O/i03zYdh0WPpbm407XFBcClMvgdKv2++pjzSgx6EtHGFTZRPPlG3nt+9vIcvnYsLQLI4tyeHimcUcW5yD9FdrlWCT3VGGjLGZB9hAsuENOPFbNpi//1ObldVug4nn2R+XMZCRa4P+qmfhpH+FVc/YH3LJ8fYAIrGMuvg4KF9y4HJzj7KZd/1OyMy3meMJ34S/d3hmiSdgdzanx1YTDZ1iA/OulTDrWjj3v2Djm/bHv3mRDQiegA1+Wxbtn09Gnj2YVK6zy63dZoeFg/bawmnftqfSG96wGUxLjf1eRsyEUDMsiXWpP+5M++Ou3gBn/MCu5zNX2cAVGGp/zBVLbXblybQ/0GCj/a7GnG4/63TbapRtH9jxDhcUHmPPNrzZ9kwoY4jdHqFme4A47d+h7LdQuxW8ObbMR51kDwIY+6Pc8YnN5MfOhc/+bjPlSNAOC7eAK8O+DhlrDwQmYgOPicQyyka7/GCjDdauDJj/Y/s9tNTsP4iPP8t+T/u2URZ860O7/XavsXc/Y+xF/NKv2+wQbCbYWmcPfmDPqCIhu/zOtJe3XXGpzSijYfvZaNj+Xf+urS9e+YwN1K11dv+48nl7xvLat+0BYfTpkDXc7qMmCkedbKswx51pt1XNZnvAGzoZTr/Tlru8zGa6To/dLm4/XP2K/Y62vGeDbu02Wwe+c4UN2pc8fuB6lJfBJ3+w1ZhFk2wZh03d/1ur/txWdY6cDYsetPXiDpfNuMfMtQeW5mo4+mS7zDFz95+pVn5mz4xP+hb8/T9seUq/bhOVzuJDNAIfPAyjTrNn2P1AA3ovffh5NfM+qWBLdROfbKslGIkyttDPxTOLuWhGMSOHZCa6iAeKhO1p+tYP7YXaWdfYLHTPWpsNujNhz6c2qwu1whNn2+z1/Adh1Ck2eBoTC0ZL7A9u2FSbaW1eZH+MJ3wTpl9x6E5bv8P+yDPzYfULNnvPLoY1L9of7Om32yC8/m8245n/Y5utnvF9W74/X2YDSWae/RHvXGmrpmZ+zR4I3n0ACifZ58PuWGYDor8QTrnFVtm0Z7Zz77YHhj9cDEUTbfa54U2bVYNdhykX23V4/HSbFZ7xAzts+Z/sWUP5Ejv8K7+2dw8bYy+kbVlkT7crltqgf+YPbdVac409uGx9356Gz73H/vAjIVutsOwPNmvb+p79QY87A6b/s60ai4btqXrVZ/agdNSJ+7/Tqg22muW4q2yVWtEUGxQ9ARtIp1xiM712q1+wB/JJF9rXByfZ4Tcvs9t51XN2ORPOtd/h7jW2XI17Ylm1x1ZjFE2yQbJyvQ24Uy6xZ5UVy2ygdrrtftSxrGCz/p0rbIAEe0a3Y5k9oDhddjm719gDYrwXAiMhW19fPOvQ5bXrrIqrL5qq7f4R6GUPsNFYojNs2v6AfwRoQD8MdS0hXlu1kxeXVbB4i61Ly/Q48bgczByZy5dnFnPm5KFkepKowVA0ekR3wF4JNkHtdhuQ24Ojw2WHL/iJfT/7elvvCXZajA3+sP/g1p2GXfbAcPAPONRiD2rtB4F4RCO22iFr2IHDjbGn+x6/rR8/7iobyOMVz3p0ZesHNqMuPq5vn1eDmgb0frK9ppmXV+ygujFISyjCu+v3sKOuFY/LwTFDA4wpCHDCmCHUNAYJRqJMGZFD6ag8CgLeRBddKZUiugvoSZRWJt7IIZncNHf/lelo1LBkSw1vfLqbzysbWbihkpdX7ADA6RAiUYPH5eDLM0ZwzNAs5k4sorY5iNflZGpxL7JApZSKg2bo/SgYjlLd1EZOhhuHCGt21POHD7fwzro91LeGD5j21HEFTB+ZgyDUt4bIzfRw8th8ji3Oob41xPAc7elRKXUorXIZBD7b3cDy7bUUZnlZu7OeZ5ZsZ2tNMwJk+dw0toX339wETC3O5uh8P7XNQaaOyGHW0XkM8XuYMiKHitpmWoJRCrI8DMv29V/rG6XUoKcBfZAyxmAMOBxCQ2uI9zdWs35XA26X8M7aPexpsNn+ul31hCKdb6eSvAyyfW4mDsvi7KnDyMlwU9nQxifbaplWksPJ4/LZXtOC2yk4RMj0ODlqSCYup70ourcpiN/rwuPaf5F0T0MrThHyte5fqUFHA3qSawlGWLurnqqGNlaU1zK6IEC2z8WO2hY+3lxDSyjCiu217G0O7fuMyyGEo51v25FDMjhv6nDW7Kjn/c+rGJXv58bTx+JwCM3BMP/z+noyPE5+dVUpRw/xk+VzUd0UpK4lSMDrpjkYJjvDrRd7lUoADehpIBSJsmSzbVaZk+lmXFGAZVtrWbernuE5GTgEDPahH88vreCT7XsZluPjvKnDeX5ZBVWNbfvmNbbQT1VjkLoWe4Bov8DbkQjMHJnLzKPyaAlF8LmcTB+ZQzAcZeOeRkryMoga2FrdzBeOKWBsYYCK2ha2Vjdx4ph88vwevC4HHqdDq4yU6gUN6OoQ4UgUp0MQEVqCkQMC+rAcH7vqWlm8uYa9zUH2NgfJ8rkZnuOjqS1CpsfJ1upmXl+zi81VTfi9ThrbwrSG7J2NHQ8Abqd0WV0E9sCQ4XYyxO+hMMtLQcBLls/F1upmyvc2c8rYAgA8Lgc5mW5mjsyjLRxhS1UzGR4HRVk+wlHD0GwvTW1hCgJeWkIRirJ8GAxNbWEcIrgcDlxOoSQvgyxf32+7VirRNKCrAReKRNlU2UQkapg4LIs9DW20hiIMzfZRtrWGir0tFGV7GZ6TwbJte2kJRmgLR2kLRWgORqhuClLZ0EZVYxt1LSFGDsmkIODh3fWVZGe4iUQNtc0hgpHD66HR6RCmleTgFGFbTTN1LXaehQEvp44vYHxRFp/urGdNRR0RY5h1dB55mR6yfW6yfC7mr9/DJ9tqOX1CIX6Pk8nDs8n0upg5MpeiLB8bKxtZvLmGmqY28gNeRuZl8sWJRWR4nESjhk1VjVQ2BAl4XeRmusnNdON2Oli+vZYMt5OoMYwrCpDlc2OM6fLspeP1F5VeNKCrlBAMR1lRXktOhpuj8zNpC0fZU9+GyyFU1LYQ8LrYXd+K32uvL3hcDnIzPUSjhkjUEIxE+XRHPR98XgXAuKIAuZm26mdbTTPvrN1DQ1uY4twMpozIJhw1fLqjnobWEE1B2/9JpsfJqeMKWLOjnqZgmNoO1y068roctIXtwcfndhDwumgLRWloC3c6fUcOgWHZPqoag0SNYcbIXAoCXlrDEVpDEVqCETbsaaQ5GGFsoZ+h2T5GFfiZNDybd9dX4nIIY4v81LWEqG0OYYAhmR7qW0NEoobxRVlk+Vz7/gAqG4O0hSIUZnkpzPKSl+mhORhhe43tnnlEbgZD/B7qWkKU721me00zs0fnM2l4FgGviw8+r+Z7L62mvKaFSSOyuePsCZw8rqDbbVnZ2Ea+30MwEmVXXSsZbicleRmdHsTawhEcIridg/QO5yNIA7pScWgNRWgLRcnJPLRKJhyJ0tAaxuNy4PfaIBiNGqqa2qhvCbNiey01TUFK8jI4aWw+uZkeGtvCrK6o4/U1u2gLR3GKPTsYkZtBU1uY2pYQtc1BGtsiTCvOwWAz79U76imvaSY/YLsC/nhzDa2hCD63E5/LidftYFS+n7xMN8vL62hsDbF2ZwMtsYAsQFVjG7mZHnIz3Bhgb3OQDLcTAXbUtR6yfoejPf6OKfAzZ0IRb6/dzZbqZiYOyyIv04PDAeGIIRw1BMNR9jS0sqehbV9XLB1D0IShWeRkusn3e3A4hIq9LVTUtlDZYKsEA14X00pymHlULoKwq76Vmqbgviq+llCESNQwLMfH8Gwfw3J85Ac8ZLhdLNu2lw8+r8IpQp7fw6h8P41tYU4YPYTiWGux2uZQbB5RNuxupDgvg5XldTS0hnl/YxVH52cyY2Qu00pymT4yh+JcewDaUdvCpsom1u2qZ+GGKkbnZ1KU7ePEMUOYOTIPh0MwxrC7vo2NexopyvZyzNBedAVxwPetAV2plNYailDXEmKI34PLId1Wx4QiURpbwzS2halvDWEMFGV58bgcVDUG2dPQSl1zCI/LwdH5fsBQUdvK3qYgOZluirK8jMjJYPGWGjZXNdHUFsblcHDNKaPIyXDTEozwx4+2smhjFa3BCBFjcDoEl8Nm2EVZXorzMijK8sXOqJwMy8mIXbAvx+EQqhrbcIpQnJdBcW4GI3IzcIhQ0xRk/vo9lO9twRjDEL+XYTlenA4Hxhh8LidOh7C7vpWdda20hPb3LOkQOGlsPm6ng/K9LWyvaSbD4+zyLKtdhtvO89RxBeysa2HtzoZ9VX+5mW68Lge76/dfgxpd4GdHbcu+M7Rsn4v8gJfd9a00x870rjt1NPdcMLlP21oDulIq7RhjqG8JU93URnMwQp7fQ3FuxgHjowa2VDexu76V+pYweZlufG4n4WiUMQUBttU0M2VE9r77NsBW/6zf1cCK8jrW7qynNWTPsCYMy2ZUQSbDczL2PUDnnbV7WLKlhrqWEIVZXkYX+BlXGGDi8GyG+Pv2MBYN6EoplSK6C+h6hUEppVJEXAFdRM4RkfUislFE7uxkvFdEno6N/1hERvV7SZVSSnWrx4AuIk7g58C5wGTgChE5uDb/G8BeY8w44CHgoIdEKqWUGmjxZOizgY3GmE3GmCDwF+Cig6a5CIg9kpzngDNE7+dWSqkjKp6AXgxs7/B/eWxYp9MYY8JAHZB/8IxE5HoRKRORssrKyr6VWCmlVKeO6EVRY8zjxphSY0xpYWEvH8iqlFKqW/EE9ApgZIf/S2LDOp1GRFxADlDdHwVUSikVn3gC+hJgvIiMFhEPcDnw8kHTvAxcHXt/KfCOSVQDd6WUSlNx3VgkIucBPwWcwBPGmB+LyI+AMmPMyyLiA/4AzARqgMuNMZt6mGclsLWP5S4Aqvr42cFG12Vw0nUZnHRd4GhjTKd11gm7U/RwiEhZV3dKJRtdl8FJ12Vw0nXpnt4pqpRSKUIDulJKpYhkDeiPJ7oA/UjXZXDSdRmcdF26kZR16EoppQ6VrBm6Ukqpg2hAV0qpFJF0Ab2nrnwHOxHZIiKrRGS5iJTFhg0RkTdFZEPsNS/R5eyMiDwhIntEZHWHYZ2WXayHY9tppYgcl7iSH6qLdblXRCpi22Z57P6L9nH/EVuX9SJydmJKfSgRGSki80XkUxFZIyL/FhuedNulm3VJxu3iE5HFIrIiti4/jA0fHetifGOsy3FPbHj/dEFujEmaP+yNTZ8DYwAPsAKYnOhy9XIdtgAFBw37L+DO2Ps7gQcSXc4uyv4F4DhgdU9lB84D/gYIcCLwcaLLH8e63At8p5NpJ8f2NS8wOrYPOhO9DrGyDQeOi73PAj6LlTfptks365KM20WAQOy9G/g49n0/g73xEuAx4MbY+28Bj8XeXw483ZflJluGHk9XvsmoY/fDvwO+nLiidM0YsxB7J3BHXZX9IuD3xvoIyBWR4UekoHHoYl26chHwF2NMmzFmM7ARuy8mnDFmpzFmWex9A7AW2/tp0m2XbtalK4N5uxhjTGPsX3fszwBfxHYxDodul8PugjzZAno8XfkOdgZ4Q0SWisj1sWFDjTE7Y+93AUMTU7Q+6arsybqt/jVWFfFEh6qvpFiX2Gn6TGw2mNTb5aB1gSTcLiLiFJHlwB7gTewZRK2xXYzDgeWNqwvyniRbQE8FpxpjjsM+AeomEflCx5HGnnMlZVvSZC57zKPAWGAGsBP434SWphdEJAA8D9xqjKnvOC7Ztksn65KU28UYEzHGzMD2UDsbmDjQy0y2gB5PV76DmjGmIva6B3gRu6F3t5/2xl73JK6EvdZV2ZNuWxljdsd+hFHgV+w/fR/U6yIibmwA/JMx5oXY4KTcLp2tS7Jul3bGmFpgPnAStorLFRvVsbz90gV5sgX0eLryHbRExC8iWe3vgbOA1RzY/fDVwEuJKWGfdFX2l4GrYq0qTgTqOlQBDEoH1SVfjN02YNfl8lhLhNHAeGDxkS5fZ2L1rL8B1hpjHuwwKum2S1frkqTbpVBEcmPvM4AzsdcE5mO7GIdDt8vhd0Ge6KvBfbh6fB726vfnwN2JLk8vyz4Ge1V+BbCmvfzYurK3gQ3AW8CQRJe1i/I/hT3lDWHr/77RVdmxV/l/HttOq4DSRJc/jnX5Q6ysK2M/sOEdpr87ti7rgXMTXf4O5ToVW52yElge+zsvGbdLN+uSjNtlGvBJrMyrge/Hho/BHnQ2As8C3thwX+z/jbHxY/qyXL31XymlUkSyVbkopZTqggZ0pZRKERrQlVIqRWhAV0qpFKEBXSmlUoQGdKWUShEa0JVSKkX8fys6RRif/ApbAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Loss Curve\n",
    "plt.plot(history.history['loss'], label = 'Training Loss')\n",
    "plt.plot(history.history['val_loss'], label = \"Test Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b0c193ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#accuracy Curve\n",
    "plt.plot(history.history['accuracy'], label = 'Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = \"Test Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806270f7",
   "metadata": {},
   "source": [
    "## 4.2 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f86c3441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2366/2366 [==============================] - 7s 3ms/step - loss: 0.3344 - accuracy: 0.9329\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741a02aa",
   "metadata": {},
   "source": [
    "##  4.3 Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e8465627",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "68ff1c6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.9875736e-01, 8.4925195e-10, 4.6798087e-07, ..., 1.3406317e-07,\n",
       "        4.6834384e-07, 3.0169909e-07],\n",
       "       [5.8569482e-14, 5.7415215e-09, 1.8645388e-10, ..., 4.8435869e-09,\n",
       "        4.5505043e-08, 5.9628411e-07],\n",
       "       [8.7432164e-01, 7.9969509e-04, 5.4046488e-04, ..., 3.9412608e-08,\n",
       "        3.7475536e-07, 3.0587980e-11],\n",
       "       ...,\n",
       "       [3.2247972e-06, 3.9425330e-10, 2.1360877e-12, ..., 1.6188505e-08,\n",
       "        4.9881891e-08, 3.7787277e-07],\n",
       "       [9.6493894e-01, 3.5509563e-03, 2.6122790e-02, ..., 6.6702512e-07,\n",
       "        2.3876087e-06, 5.1484044e-08],\n",
       "       [2.3166494e-07, 1.6962098e-11, 1.0858498e-07, ..., 4.0673681e-10,\n",
       "        1.9530974e-10, 1.2601041e-07]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2db297",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b5d9df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cd8a46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
